# Python Kurzus: AI √Ågensek Programoz√°sa

**K√©sz√ºlt**: 2026. janu√°r 20.  
**C√©lk√∂z√∂ns√©g**: Halad√≥ Python fejleszt≈ëk, AI m√©rn√∂k√∂k  
**El≈ëfelt√©tel**: Alapvet≈ë Python ismeretek, API tapasztalat  
**Id≈ëtartam**: 8-10 √≥ra gyakorlati anyag

---

## üìã Tartalomjegyz√©k

1. [Aszinkron Python (async/await)](#1-aszinkron-python-asyncawait)
2. [Decoratorok √©s Metaprogramoz√°s](#2-decoratorok-√©s-metaprogramoz√°s)
3. [Type Hints √©s Protocol-ok](#3-type-hints-√©s-protocol-ok)
4. [Context Managerek](#4-context-managerek)
5. [Dependency Injection √©s Factory Pattern](#5-dependency-injection-√©s-factory-pattern)
6. [Pydantic √©s Adatvalid√°ci√≥](#6-pydantic-√©s-adatvalid√°ci√≥)
7. [FastAPI √©s REST API-k](#7-fastapi-√©s-rest-api-k)
8. [LangGraph √©s Workflow Orchestration](#8-langgraph-√©s-workflow-orchestration)
9. [Observability √©s Metrics](#9-observability-√©s-metrics)
10. [Best Practices AI √Ågensekn√©l](#10-best-practices-ai-√°gensekn√©l)

---

## 1. Aszinkron Python (async/await)

### 1.1 Mi az az Aszinkron Programoz√°s?

Az aszinkron programoz√°s lehet≈ëv√© teszi, hogy **egyidej≈±leg t√∂bb m≈±veletet is v√©grehajtsunk** an√©lk√ºl, hogy megv√°rn√°nk az egyik befejez√©s√©t.

**Mi√©rt fontos AI √°gensekn√©l?**
- LLM API h√≠v√°sok lass√∫ak (1-5 m√°sodperc)
- T√∂bb node p√°rhuzamosan futhat
- I/O-bound m≈±veletek (h√°l√≥zat, f√°jl, cache)

### 1.2 Alapok: async def √©s await

**F√°jl**: `app/nodes/triage_node.py`

```python
async def execute(self, state: AgentState) -> Dict:
    """
    Async function - nem blokkolja a programot.
    """
    # await = "v√°rj meg, de k√∂zben m√°s is futhat"
    cached_result = await self.cache.get(cache_key)
    
    if cached_result is not None:
        return {"classification": cached_result}
    
    # Async LLM h√≠v√°s
    response = await self.llm_client.complete(
        prompt=prompt,
        model=self.model_name,
        max_tokens=10
    )
    
    return {"classification": response.content}
```

**Magyar√°zat:**
- `async def` = aszinkron f√ºggv√©ny defin√≠ci√≥
- `await` = v√°r az eredm√©nyre, de **nem blokkolja** a thread-et
- M√°s async m≈±veletek futhatnak k√∂zben

### 1.3 Gyakorlati P√©lda: P√°rhuzamos API H√≠v√°sok

```python
import asyncio

async def fetch_model_cheap(query: str):
    """Olcs√≥ modell h√≠v√°s - gyors (0.5s)"""
    await asyncio.sleep(0.5)  # Szimul√°ci√≥
    return "cheap_response"

async def fetch_model_expensive(query: str):
    """Dr√°ga modell h√≠v√°s - lass√∫ (2s)"""
    await asyncio.sleep(2.0)
    return "expensive_response"

# ‚ùå ROSSZ: Szekvenci√°lis - 2.5s
async def bad_approach():
    cheap = await fetch_model_cheap("query")
    expensive = await fetch_model_expensive("query")
    return cheap, expensive

# ‚úÖ J√ì: P√°rhuzamos - 2s (csak a leglassabb)
async def good_approach():
    results = await asyncio.gather(
        fetch_model_cheap("query"),
        fetch_model_expensive("query")
    )
    return results

# Futtat√°s
import asyncio
cheap, expensive = asyncio.run(good_approach())
```

**Id≈ëbeli k√ºl√∂nbs√©g:**
- Szekvenci√°lis: 0.5s + 2s = **2.5s**
- P√°rhuzamos: max(0.5s, 2s) = **2s** (20% gyorsabb)

### 1.4 Async Cache M≈±veletek

**F√°jl**: `app/cache/memory_cache.py`

```python
class MemoryCache:
    async def get(self, key: str) -> Optional[Any]:
        """
        Async get - non-blocking cache lookup.
        
        Mi√©rt async?
        - In-memory: gyors, de async interf√©sz konzisztencia
        - Redis/DB cache eset√©n: h√°l√≥zati I/O
        """
        if key not in self._cache:
            return None
        
        value, timestamp = self._cache[key]
        
        # TTL check
        if time.time() - timestamp > self._ttl:
            del self._cache[key]
            return None
        
        return value
    
    async def set(self, key: str, value: Any) -> None:
        """Async set - konzisztens interf√©sz."""
        self._cache[key] = (value, time.time())
```

**Mi√©rt async cache in-memory eset√©n?**
1. **Konzisztens interf√©sz**: Redis cache async, √≠gy minden cache async
2. **J√∂v≈ëbiztos**: k√∂nnyen cser√©lhet≈ë Redis-re
3. **Type safety**: Cache Protocol async m≈±veleteket √≠r el≈ë

### 1.5 Async Context Manager

**F√°jl**: `app/utils/timing.py`

```python
from contextlib import asynccontextmanager
import time

@asynccontextmanager
async def async_timer(callback=None):
    """
    Async context manager - id≈ëm√©r√©s.
    
    Haszn√°lat:
        async with async_timer() as timer:
            await expensive_operation()
            print(f"Took {timer['elapsed']}s")
    """
    start = time.time()
    elapsed_container = {"elapsed": 0.0}
    
    try:
        yield elapsed_container
    finally:
        elapsed = time.time() - start
        elapsed_container["elapsed"] = elapsed
        if callback:
            callback(elapsed)

# Haszn√°lat node-ban:
async def execute(self, state):
    async with async_timer() as timer:
        result = await self.llm_client.complete(...)
    
    logger.info(f"LLM call took {timer['elapsed']:.2f}s")
```

**El≈ëny√∂k:**
- Automatikus cleanup (finally blokk)
- Eleg√°ns szintaxis
- Exception-safe

### 1.6 FastAPI Async Endpoints

**F√°jl**: `app/main.py`

```python
from fastapi import FastAPI

app = FastAPI()

@app.post("/run")
async def run_agent(request: RunRequest):
    """
    Async endpoint - t√∂bb k√©r√©st is kezel p√°rhuzamosan.
    
    FastAPI automatikusan:
    - T√∂bb request p√°rhuzamosan fut
    - Non-blocking I/O
    - High throughput
    """
    # Async LangGraph h√≠v√°s
    result = await agent_graph.ainvoke(initial_state)
    
    return {"answer": result["final_answer"]}
```

**Teljes√≠tm√©ny:**
- Szinkron endpoint: 1 request/m√°sodperc
- Async endpoint: 50-100 request/m√°sodperc (I/O-bound eset√©n)

### 1.7 Gyakorl√≥ Feladatok

**Feladat 1: P√°rhuzamos Cache Check**

```python
async def check_multiple_caches(keys: List[str]) -> Dict[str, Any]:
    """
    Ellen≈ërizz t√∂bb cache key-t p√°rhuzamosan.
    
    TODO: Implement√°ld asyncio.gather() haszn√°lat√°val!
    """
    # Megold√°s:
    results = await asyncio.gather(
        *[cache.get(key) for key in keys]
    )
    return dict(zip(keys, results))
```

**Feladat 2: Timeout Kezel√©s**

```python
import asyncio

async def llm_call_with_timeout(prompt: str, timeout: float = 5.0):
    """
    LLM h√≠v√°s timeout-tal.
    
    TODO: Implement√°ld asyncio.wait_for() haszn√°lat√°val!
    """
    try:
        result = await asyncio.wait_for(
            llm_client.complete(prompt),
            timeout=timeout
        )
        return result
    except asyncio.TimeoutError:
        logger.error(f"LLM call timed out after {timeout}s")
        raise HTTPException(504, "Request timeout")
```

---

## 2. Decoratorok √©s Metaprogramoz√°s

### 2.1 Mi az a Decorator?

A decorator egy **f√ºggv√©ny, ami m√≥dos√≠t egy m√°sik f√ºggv√©nyt vagy oszt√°lyt**.

**Szintaxis:**
```python
@decorator_name
def function():
    pass

# Egyen√©rt√©k≈± ezzel:
def function():
    pass
function = decorator_name(function)
```

### 2.2 Context Manager Decoratorok

**F√°jl**: `app/utils/timing.py`

```python
from contextlib import contextmanager

@contextmanager
def timer(callback=None):
    """
    @contextmanager decorator - egyszer≈±s√≠ti a context manager √≠r√°st.
    
    N√©lk√ºle:
        class Timer:
            def __enter__(self): ...
            def __exit__(self): ...
    
    Vele:
        Csak egy f√ºggv√©ny generator-ral.
    """
    start = time.time()
    elapsed_container = {"elapsed": 0.0}
    
    try:
        yield elapsed_container  # Itt fut a with blokk tartalma
    finally:
        # Cleanup - mindig lefut
        elapsed = time.time() - start
        elapsed_container["elapsed"] = elapsed
```

**Hogyan m≈±k√∂dik?**
1. `@contextmanager` = decorator, ami gener√°torb√≥l context managert csin√°l
2. `yield` el≈ëtti r√©sz = `__enter__`
3. `yield` ut√°ni r√©sz = `__exit__`

### 2.3 Async Context Manager Decorator

```python
from contextlib import asynccontextmanager

@asynccontextmanager
async def async_timer(callback=None):
    """
    Async verzi√≥ - await-elhet≈ë m≈±veletekhez.
    """
    start = time.time()
    elapsed = {"value": 0.0}
    
    try:
        yield elapsed
    finally:
        elapsed["value"] = time.time() - start
        if callback:
            callback(elapsed["value"])

# Haszn√°lat:
async def node_execution():
    async with async_timer(lambda t: logger.info(f"Took {t}s")):
        result = await llm_client.complete(prompt)
```

**Mi√©rt kell az async verzi√≥?**
- `yield` k√∂r√ºl async m≈±veletek lehetnek
- Cleanup f√°zisban is lehet await

### 2.4 FastAPI Route Decoratorok

**F√°jl**: `app/main.py`

```python
from fastapi import FastAPI

app = FastAPI()

# @app.post = route decorator
@app.post("/run", response_model=RunResponse)
async def run_agent(request: RunRequest):
    """
    @app.post decorator hat√°sai:
    1. Regisztr√°lja az endpoint-ot
    2. POST method
    3. /run URL path
    4. Automatic request validation (RunRequest)
    5. Automatic response validation (RunResponse)
    6. OpenAPI dokument√°ci√≥ gener√°l√°s
    """
    return await process_request(request)

# Egyen√©rt√©k≈± ezzel:
async def run_agent(request: RunRequest):
    return await process_request(request)

app.add_api_route(
    "/run",
    run_agent,
    methods=["POST"],
    response_model=RunResponse
)
```

**T√∂bb decorator kombin√°l√°sa:**

```python
@app.post("/run")
@cache_response(ttl=60)  # Custom decorator
@rate_limit(requests=100, window=60)  # Custom decorator
async def run_agent(request: RunRequest):
    """Decoratorok alulr√≥l felfel√© hajt√≥dnak v√©gre."""
    return result
```

### 2.5 Dataclass Decorator

**F√°jl**: `app/cache/memory_cache.py`

```python
from dataclasses import dataclass

@dataclass
class CacheEntry:
    """
    @dataclass decorator automatikusan gener√°l:
    - __init__()
    - __repr__()
    - __eq__()
    - __hash__() (ha frozen=True)
    """
    value: Any
    timestamp: float
    ttl: int

# Haszn√°lat:
entry = CacheEntry(value="result", timestamp=time.time(), ttl=3600)
print(entry)  # CacheEntry(value='result', timestamp=1234567890.0, ttl=3600)
```

**El≈ëny√∂k:**
- Kevesebb boilerplate k√≥d
- Type hints t√°mogat√°s
- Immutable oszt√°lyok (frozen=True)

### 2.6 Lifespan Context Manager (FastAPI)

**F√°jl**: `app/main.py`

```python
from contextlib import asynccontextmanager
from fastapi import FastAPI

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan manager.
    
    Startup:
        - LLM client inicializ√°l√°s
        - Cache setup
        - DB kapcsolat
    
    Shutdown:
        - Connection lez√°r√°s
        - Cache flush
        - Cleanup
    """
    # STARTUP - yield el≈ëtt
    logger.info("Starting application...")
    
    global llm_client, cache
    llm_client = OpenAIClient(api_key=settings.openai_api_key)
    cache = MemoryCache(ttl_seconds=3600)
    
    logger.info("Application ready")
    
    yield  # Itt fut az alkalmaz√°s
    
    # SHUTDOWN - yield ut√°n
    logger.info("Shutting down...")
    await cache.clear()
    logger.info("Cleanup complete")

# Alkalmaz√°s setup
app = FastAPI(lifespan=lifespan)
```

**El≈ëny√∂k:**
- K√∂zpontos√≠tott setup/teardown
- Exception safety
- Clean code

### 2.7 Custom Decorator P√©lda: Retry Logic

```python
import functools
import asyncio
from typing import Callable

def async_retry(max_retries: int = 3, delay: float = 1.0):
    """
    Retry decorator async f√ºggv√©nyekhez.
    
    Haszn√°lat:
        @async_retry(max_retries=3, delay=2.0)
        async def unstable_api_call():
            ...
    """
    def decorator(func: Callable):
        @functools.wraps(func)  # Meg≈ërzi az eredeti f√ºggv√©ny metadat√°it
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    logger.warning(
                        f"Attempt {attempt + 1}/{max_retries} failed: {e}"
                    )
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay * (2 ** attempt))  # Exponential backoff
            
            # Ha minden retry failed
            raise last_exception
        
        return wrapper
    return decorator

# Haszn√°lat:
@async_retry(max_retries=5, delay=1.0)
async def call_openai_api(prompt: str):
    response = await openai.chat.completions.create(...)
    return response
```

### 2.8 Gyakorl√≥ Feladatok

**Feladat 1: Logging Decorator**

```python
def log_execution(func):
    """
    TODO: √çrj decoratort, ami logolja a f√ºggv√©ny nev√©t √©s fut√°si idej√©t.
    """
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        logger.info(f"Executing {func.__name__}")
        start = time.time()
        
        result = await func(*args, **kwargs)
        
        elapsed = time.time() - start
        logger.info(f"{func.__name__} took {elapsed:.2f}s")
        
        return result
    return wrapper
```

**Feladat 2: Cache Decorator**

```python
def cached(ttl: int = 3600):
    """
    TODO: √çrj cache decoratort f√ºggv√©nyekhez.
    """
    def decorator(func):
        cache_dict = {}
        
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            # Cache key gener√°l√°s args-b√≥l
            cache_key = f"{func.__name__}:{args}:{kwargs}"
            
            # Check cache
            if cache_key in cache_dict:
                value, timestamp = cache_dict[cache_key]
                if time.time() - timestamp < ttl:
                    return value
            
            # Cache miss
            result = await func(*args, **kwargs)
            cache_dict[cache_key] = (result, time.time())
            
            return result
        return wrapper
    return decorator
```

---

## 3. Type Hints √©s Protocol-ok

### 3.1 Mi√©rt Fontosak a Type Hintek?

**AI √°gens projektekn√©l kritikusak:**
1. **Code completion**: IDE seg√≠t
2. **Type safety**: Hib√°k a fut√°s el≈ëtt
3. **Documentation**: Self-documenting code
4. **Refactoring**: Biztons√°gos v√°ltoztat√°sok

### 3.2 Alapvet≈ë Type Hints

**F√°jl**: `app/nodes/triage_node.py`

```python
from typing import Dict, List, Optional, Any

async def execute(self, state: AgentState) -> Dict:
    """
    Type hints minden param√©terhez √©s return √©rt√©khez.
    
    state: AgentState - custom TypedDict
    -> Dict - visszat√©r√©si √©rt√©k t√≠pusa
    """
    classification: str = "simple"  # Lok√°lis v√°ltoz√≥ hint
    cache_key: Optional[str] = None  # Lehet None is
    
    return {"classification": classification}
```

**T√≠pus kateg√≥ri√°k:**
- `str`, `int`, `float`, `bool` - primit√≠vek
- `List[str]` - lista string-ekb≈ël
- `Dict[str, int]` - dictionary
- `Optional[str]` - lehet str vagy None
- `Any` - b√°rmilyen t√≠pus (ker√ºlend≈ë)

### 3.3 Protocol - Struktur√°lis T√≠pusrendszer

**F√°jl**: `app/cache/interfaces.py`

```python
from typing import Protocol, Optional, Any

class Cache(Protocol):
    """
    Protocol = interf√©sz Python-ban.
    
    Nem kell explicit implement√°lni (duck typing),
    el√©g ha a met√≥dusok megvannak.
    """
    
    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache."""
        ...
    
    async def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:
        """Set value in cache."""
        ...
    
    async def delete(self, key: str) -> None:
        """Delete key from cache."""
        ...
    
    async def clear(self) -> None:
        """Clear all cached items."""
        ...
```

**Implement√°ci√≥:**

```python
class MemoryCache:
    """
    NEM kell: class MemoryCache(Cache)
    
    El√©g ha a met√≥dusok stimmelnek!
    """
    
    async def get(self, key: str) -> Optional[Any]:
        return self._cache.get(key)
    
    async def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:
        self._cache[key] = value
    
    # ... t√∂bbi met√≥dus
```

**Haszn√°lat:**

```python
def create_node(cache: Cache):  # Protocol type hint
    """
    Elfogad B√ÅRMILYEN objektumot, ami megfelel a Cache protocol-nak.
    
    Lehet:
    - MemoryCache
    - RedisCache
    - FileCache
    
    Mindegy, csak a met√≥dusok legyenek meg!
    """
    return TriageNode(cache=cache)
```

**El≈ëny√∂k Protocol haszn√°lata:**
- **Dependency Inversion Principle**: F√ºgg√ºnk az absztrakci√≥t√≥l, nem a konkr√©t implement√°ci√≥t√≥l
- **Testability**: K√∂nny≈± mockolni
- **Flexibility**: K√∂nny≈± cser√©lni az implement√°ci√≥t

### 3.4 LLM Client Protocol

**F√°jl**: `app/llm/interfaces.py`

```python
from typing import Protocol, Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class CompletionResponse:
    """Response model LLM h√≠v√°sokhoz."""
    content: str
    input_tokens: int
    output_tokens: int
    latency_seconds: float
    model: str

class LLMClient(Protocol):
    """
    Protocol minden LLM client-hez.
    
    Implement√°ci√≥k:
    - OpenAIClient (production)
    - MockLLMClient (testing)
    - AnthropicClient (j√∂v≈ëbeli)
    """
    
    async def complete(
        self,
        prompt: str,
        model: str,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> CompletionResponse:
        """Complete text prompt."""
        ...
```

**Haszn√°lat node-okban:**

```python
class TriageNode:
    def __init__(self, llm_client: LLMClient):
        """
        llm_client: LLMClient protocol
        
        Runtime-ban lehet:
        - OpenAIClient (API key van)
        - MockLLMClient (nincs API key)
        """
        self.llm_client = llm_client
    
    async def execute(self, state):
        response = await self.llm_client.complete(
            prompt="Classify this",
            model="gpt-3.5-turbo",
            max_tokens=10
        )
        return response.content
```

### 3.5 TypedDict - Struktur√°lt Dictionary

**F√°jl**: `app/graph/state.py`

```python
from typing_extensions import TypedDict
from typing import List, Dict, Any, Optional

class AgentState(TypedDict, total=False):
    """
    TypedDict = t√≠pusozott dictionary.
    
    total=False = mindegyik mez≈ë optional
    
    El≈ëny√∂k:
    - IDE autocomplete m≈±k√∂dik
    - Type checker l√°tja a hib√°kat
    - Self-documenting
    """
    user_input: str
    classification: Optional[str]
    retrieved_docs: List[str]
    retrieval_context: Optional[str]
    reasoning_output: Optional[str]
    final_answer: Optional[str]
    nodes_executed: List[str]
    models_used: List[str]
    timings: Dict[str, float]
    cache_hits: Dict[str, int]

# Haszn√°lat:
def process_state(state: AgentState) -> AgentState:
    """
    IDE tudja, hogy state["user_input"] string!
    
    state["typo"]  # ‚Üê Type checker error!
    """
    print(state["user_input"])  # ‚úÖ OK
    print(state["invalid_key"])  # ‚ùå Type error
    
    return state
```

### 3.6 Literal - Konkr√©t √ârt√©kek T√≠pusa

**F√°jl**: `app/graph/agent_graph.py`

```python
from typing import Literal

def route_after_triage(state: AgentState) -> Literal["retrieval", "summary"]:
    """
    Literal["retrieval", "summary"] = csak ezek a 2 string megengedett!
    
    Visszaadhat:
    - "retrieval" ‚úÖ
    - "summary" ‚úÖ
    - "something_else" ‚ùå Type error!
    """
    classification = state.get("classification")
    
    if classification == "simple":
        return "summary"  # ‚úÖ OK
    
    return "retrieval"  # ‚úÖ OK
    
    # return "invalid"  # ‚ùå Type checker hiba!
```

**Haszn√°lat LangGraph-n√°l:**

```python
from langgraph.graph import StateGraph

workflow = StateGraph(AgentState)

# Conditional edges mapping
workflow.add_conditional_edges(
    "triage",
    route_after_triage,  # Literal return type
    {
        "retrieval": "retrieval",  # Literal nevek kell
        "summary": "summary"
    }
)
```

**El≈ëny√∂k:**
- **Type safety**: Nem lehet el√≠rni a routing target-et
- **Autocomplete**: IDE javasolja a lehet≈ës√©geket
- **Refactoring**: K√∂nny≈± √°tnevezni node-okat

### 3.7 Generic Types

```python
from typing import TypeVar, Generic, List

T = TypeVar('T')  # Generic type variable

class CacheWrapper(Generic[T]):
    """
    Generic cache - b√°rmilyen t√≠pushoz.
    
    Haszn√°lat:
        cache: CacheWrapper[str] = CacheWrapper()
        cache: CacheWrapper[int] = CacheWrapper()
    """
    
    def __init__(self):
        self._storage: Dict[str, T] = {}
    
    async def get(self, key: str) -> Optional[T]:
        return self._storage.get(key)
    
    async def set(self, key: str, value: T) -> None:
        self._storage[key] = value

# Haszn√°lat:
string_cache: CacheWrapper[str] = CacheWrapper()
await string_cache.set("key", "value")  # ‚úÖ OK
await string_cache.set("key", 123)  # ‚ùå Type error - int nem str!

int_cache: CacheWrapper[int] = CacheWrapper()
await int_cache.set("key", 123)  # ‚úÖ OK
```

### 3.8 Gyakorl√≥ Feladatok

**Feladat 1: Protocol Implement√°ci√≥**

```python
from typing import Protocol

class MetricsCollector(Protocol):
    """
    TODO: Defini√°lj Protocol-t metrik√°k gy≈±jt√©s√©hez.
    
    Met√≥dusok:
    - record_count(name: str, value: int)
    - record_latency(name: str, seconds: float)
    - get_metrics() -> Dict[str, Any]
    """
    pass

# Implement√°ci√≥:
class PrometheusCollector:
    """TODO: Implement√°ld a Protocol-t."""
    pass
```

**Feladat 2: TypedDict State**

```python
from typing_extensions import TypedDict
from typing import List, Optional

class WorkflowState(TypedDict, total=False):
    """
    TODO: Defini√°lj state-et egy egyszer≈± workflow-hoz.
    
    Mez≈ëk:
    - task_name: str
    - status: Literal["pending", "running", "complete", "failed"]
    - result: Optional[str]
    - error: Optional[str]
    - start_time: float
    - end_time: Optional[float]
    """
    pass
```

---

## 4. Context Managerek

### 4.1 Mi az a Context Manager?

A context manager **automatikus resource kezel√©st** biztos√≠t: setup √©s cleanup.

**Klasszikus p√©lda:**

```python
# ‚ùå ROSSZ - file leak vesz√©ly
file = open("data.txt")
data = file.read()
# Ha exception van, file nem z√°r√≥dik le!

# ‚úÖ J√ì - with statement
with open("data.txt") as file:
    data = file.read()
# Automatikusan lez√°rja, m√©g exception eset√©n is!
```

### 4.2 Timing Context Manager

**F√°jl**: `app/utils/timing.py`

```python
from contextlib import contextmanager
import time

@contextmanager
def timer(callback=None):
    """
    Id≈ëm√©r≈ë context manager.
    
    Haszn√°lat:
        with timer(lambda t: print(f"Took {t}s")):
            expensive_operation()
    """
    start = time.time()
    elapsed_container = {"elapsed": 0.0}
    
    try:
        yield elapsed_container  # Visszaadja a container-t
    finally:
        # MINDIG lefut, m√©g exception eset√©n is!
        elapsed = time.time() - start
        elapsed_container["elapsed"] = elapsed
        
        if callback:
            callback(elapsed)

# Haszn√°lat:
with timer() as t:
    process_data()
    print(f"Processing took {t['elapsed']:.2f}s")
```

**Hogyan m≈±k√∂dik?**
1. `__enter__`: `start = time.time()` + `yield`
2. `with` blokk k√≥dja fut
3. `__exit__`: `finally` blokk (elapsed sz√°m√≠t√°s, callback)

### 4.3 Async Timing Context Manager

**Ugyanaz, async verzi√≥ban:**

```python
from contextlib import asynccontextmanager

@asynccontextmanager
async def async_timer(callback=None):
    """
    Async timing - await-elhet≈ë m≈±veletekhez.
    """
    start = time.time()
    elapsed = {"value": 0.0}
    
    try:
        yield elapsed
    finally:
        elapsed["value"] = time.time() - start
        if callback:
            callback(elapsed["value"])

# Haszn√°lat:
async def node_execute():
    async with async_timer(lambda t: logger.info(f"Node took {t}s")):
        result = await llm_client.complete(prompt)
    
    return result
```

### 4.4 Gyakorlati P√©lda: Node Id≈ëm√©r√©s

**F√°jl**: `app/nodes/triage_node.py`

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute triage with automatic timing."""
    
    # Context manager timing
    async with async_timer() as timer_ctx:
        # Cache check
        cached_result = await self.cache.get(cache_key)
        
        if cached_result is not None:
            classification = cached_result
        else:
            # LLM call
            response = await self.llm_client.complete(...)
            classification = response.content
    
    # Timer automatikusan friss√≠tette az elapsed-et
    logger.info(f"Triage took {timer_ctx['elapsed']:.3f}s")
    
    return {"classification": classification}
```

**El≈ëny√∂k:**
- **Exception safe**: Mindig m√©ri az id≈ët, m√©g hiba eset√©n is
- **Clean code**: Nem kell `try/finally` minden node-ban
- **Konzisztens**: Ugyanaz a pattern minden node-n√°l

### 4.5 FastAPI Lifespan Context Manager

**F√°jl**: `app/main.py`

```python
from contextlib import asynccontextmanager
from fastapi import FastAPI

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifecycle manager.
    
    Startup (yield el≈ëtt):
    - Global v√°ltoz√≥k inicializ√°l√°sa
    - DB kapcsolat
    - Cache setup
    - LLM client
    
    Shutdown (yield ut√°n):
    - Kapcsolatok lez√°r√°sa
    - Cache flush
    - Cleanup
    """
    # === STARTUP ===
    global llm_client, cache, agent_graph
    
    logger.info("üöÄ Starting application...")
    
    # LLM client init
    if settings.openai_api_key:
        llm_client = OpenAIClient(api_key=settings.openai_api_key)
    else:
        llm_client = MockLLMClient()
    
    # Cache init
    cache = MemoryCache(ttl_seconds=3600)
    
    # Agent graph
    agent_graph = create_agent_graph(
        llm_client=llm_client,
        cache=cache
    )
    
    logger.info("‚úÖ Application ready")
    
    # === RUN ===
    yield  # App runs here
    
    # === SHUTDOWN ===
    logger.info("üõë Shutting down...")
    
    await cache.clear()
    # Close DB connections
    # Flush metrics
    
    logger.info("‚úÖ Cleanup complete")

# Create app with lifespan
app = FastAPI(lifespan=lifespan)
```

**Mi√©rt fontos?**
- **Resource management**: Nem leak-elnek a connection-√∂k
- **Graceful shutdown**: Adatok nem vesznek el
- **Centraliz√°lt**: Egy helyen az √∂sszes setup/teardown

### 4.6 Custom Context Manager Class

**Klasszikus __enter__/__exit__ szintaxis:**

```python
class TransactionManager:
    """
    Database transaction manager.
    
    P√©lda class-based context manager-re.
    """
    
    def __init__(self, db_connection):
        self.db = db_connection
        self.transaction = None
    
    def __enter__(self):
        """Called when entering 'with' block."""
        self.transaction = self.db.begin_transaction()
        logger.info("Transaction started")
        return self.transaction
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Called when exiting 'with' block.
        
        Args:
            exc_type: Exception type (if exception occurred)
            exc_val: Exception value
            exc_tb: Exception traceback
            
        Returns:
            False = re-raise exception
            True = suppress exception
        """
        if exc_type is None:
            # Success - commit
            self.transaction.commit()
            logger.info("Transaction committed")
        else:
            # Error - rollback
            self.transaction.rollback()
            logger.error(f"Transaction rolled back: {exc_val}")
        
        return False  # Re-raise exception

# Haszn√°lat:
with TransactionManager(db) as txn:
    txn.insert("users", {"name": "John"})
    txn.insert("orders", {"user_id": 123})
    # Ha hiba van, automatikusan rollback!
```

### 4.7 Async Context Manager Class

```python
class AsyncDatabaseConnection:
    """Async DB connection manager."""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.connection = None
    
    async def __aenter__(self):
        """Async enter."""
        self.connection = await async_connect(self.connection_string)
        logger.info("DB connected")
        return self.connection
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async exit."""
        if self.connection:
            await self.connection.close()
            logger.info("DB connection closed")
        
        return False

# Haszn√°lat:
async with AsyncDatabaseConnection("postgresql://...") as conn:
    result = await conn.execute("SELECT * FROM users")
# Connection automatikusan lez√°rul
```

### 4.8 Gyakorl√≥ Feladatok

**Feladat 1: Rate Limiter Context Manager**

```python
@contextmanager
def rate_limit(max_requests: int, window_seconds: int):
    """
    TODO: √çrj context managert rate limiting-hez.
    
    Dobjon RateLimitError-t, ha t√∫l sok request.
    """
    # Megold√°s:
    if get_request_count(window_seconds) >= max_requests:
        raise RateLimitError("Too many requests")
    
    increment_request_count()
    
    try:
        yield
    finally:
        # Cleanup
        pass

# Haszn√°lat:
with rate_limit(max_requests=100, window_seconds=60):
    process_request()
```

**Feladat 2: Metrics Context Manager**

```python
@asynccontextmanager
async def track_metrics(operation_name: str):
    """
    TODO: √çrj context managert metrik√°k automatikus r√∂gz√≠t√©s√©hez.
    
    - M√©rje az id≈ët
    - Sz√°ml√°lja a h√≠v√°sokat
    - R√∂gz√≠tse az errorokat
    """
    start = time.time()
    
    try:
        yield
        # Success
        metrics.count(f"{operation_name}_success").inc()
    except Exception as e:
        # Error
        metrics.count(f"{operation_name}_error").inc()
        raise
    finally:
        elapsed = time.time() - start
        metrics.histogram(f"{operation_name}_latency").observe(elapsed)
```

---

## 5. Dependency Injection √©s Factory Pattern

### 5.1 Mi az a Dependency Injection (DI)?

**Dependency Injection** = f√ºgg≈ës√©gek k√≠v√ºlr≈ël val√≥ √°tad√°sa konstruktoron kereszt√ºl.

**‚ùå ROSSZ - Hard-coded dependency:**

```python
class TriageNode:
    def __init__(self):
        # Hard-coded - nem tesztelhet≈ë!
        self.llm_client = OpenAIClient(api_key="sk-...")
        self.cache = MemoryCache()
```

**‚úÖ J√ì - Dependency Injection:**

```python
class TriageNode:
    def __init__(
        self,
        llm_client: LLMClient,  # Protocol!
        cache: Cache  # Protocol!
    ):
        # K√≠v√ºlr≈ël kapjuk - tesztelhet≈ë!
        self.llm_client = llm_client
        self.cache = cache
```

**El≈ëny√∂k:**
1. **Testability**: Mock objektumokat adhatunk √°t
2. **Flexibility**: K√∂nnyen cser√©lhet≈ë az implement√°ci√≥
3. **SOLID principles**: Dependency Inversion Principle

### 5.2 Node Dependency Injection

**F√°jl**: `app/nodes/triage_node.py`

```python
from app.llm.interfaces import LLMClient
from app.cache.interfaces import Cache
from app.llm.cost_tracker import CostTracker
from app.llm.models import ModelSelector

class TriageNode:
    """
    Triage node with full DI.
    
    Minden dependency Protocol t√≠pus√∫!
    """
    
    def __init__(
        self,
        llm_client: LLMClient,  # ‚Üê Interf√©sz, nem konkr√©t oszt√°ly!
        cost_tracker: CostTracker,
        model_selector: ModelSelector,
        cache: Cache  # ‚Üê Interf√©sz!
    ):
        """
        Constructor injection.
        
        Args:
            llm_client: LLM client protocol
            cost_tracker: Cost tracking service
            model_selector: Model selection service
            cache: Cache protocol
        """
        self.llm_client = llm_client
        self.cost_tracker = cost_tracker
        self.model_selector = model_selector
        self.cache = cache
        
        # Model selection AFTER injection
        self.model_name = model_selector.get_model_name(ModelTier.CHEAP)
    
    async def execute(self, state: AgentState) -> Dict:
        """Execute using injected dependencies."""
        # Use protocol methods
        cached = await self.cache.get(key)
        response = await self.llm_client.complete(prompt, self.model_name)
        
        return {"classification": response.content}
```

**Haszn√°lat:**

```python
# Production
llm_client = OpenAIClient(api_key=settings.api_key)
cache = MemoryCache(ttl_seconds=3600)

node = TriageNode(
    llm_client=llm_client,
    cost_tracker=cost_tracker,
    model_selector=model_selector,
    cache=cache
)

# Testing
mock_llm = MockLLMClient()
mock_cache = MockCache()

test_node = TriageNode(
    llm_client=mock_llm,  # Mock!
    cost_tracker=mock_tracker,
    model_selector=mock_selector,
    cache=mock_cache  # Mock!
)
```

### 5.3 Factory Pattern

**F√°jl**: `app/graph/agent_graph.py`

```python
class AgentGraphFactory:
    """
    Factory for creating agent graphs.
    
    Centraliz√°lt dependency management √©s graph assembly.
    """
    
    def __init__(
        self,
        llm_client: LLMClient,
        model_selector: ModelSelector,
        cost_tracker: CostTracker,
        node_cache: Cache,
        embedding_cache: Cache
    ):
        """
        Factory constructor - minden dependency itt.
        """
        self.llm_client = llm_client
        self.model_selector = model_selector
        self.cost_tracker = cost_tracker
        self.node_cache = node_cache
        self.embedding_cache = embedding_cache
    
    def create_graph(self):
        """
        Create and assemble the full agent graph.
        
        Ez a "composition root" - itt t√∂rt√©nik minden wiring.
        """
        # Create nodes with DI
        triage_node = TriageNode(
            llm_client=self.llm_client,
            cost_tracker=self.cost_tracker,
            model_selector=self.model_selector,
            cache=self.node_cache
        )
        
        retrieval_node = RetrievalNode(
            llm_client=self.llm_client,
            cost_tracker=self.cost_tracker,
            model_selector=self.model_selector,
            embedding_cache=self.embedding_cache
        )
        
        reasoning_node = ReasoningNode(
            llm_client=self.llm_client,
            cost_tracker=self.cost_tracker,
            model_selector=self.model_selector
        )
        
        summary_node = SummaryNode(
            llm_client=self.llm_client,
            cost_tracker=self.cost_tracker,
            model_selector=self.model_selector
        )
        
        # Build LangGraph workflow
        workflow = StateGraph(AgentState)
        workflow.add_node("triage", triage_node.execute)
        workflow.add_node("retrieval", retrieval_node.execute)
        workflow.add_node("reasoning", reasoning_node.execute)
        workflow.add_node("summary", summary_node.execute)
        
        # Add edges...
        workflow.set_entry_point("triage")
        # ... routing logic ...
        
        # Compile and return
        return workflow.compile()
```

**El≈ëny√∂k:**
- **Separation of Concerns**: Factory != business logic
- **Centraliz√°lt wiring**: Egy helyen minden dependency
- **Tesztelhet≈ës√©g**: Factory-t mockolni k√∂nny≈±

### 5.4 Convenience Factory Function

**F√°jl**: `app/graph/agent_graph.py`

```python
def create_agent_graph(
    llm_client: LLMClient,
    model_selector: ModelSelector,
    cost_tracker: CostTracker,
    node_cache: Cache,
    embedding_cache: Cache
):
    """
    Convenience function - egyszer≈±s√≠ti a haszn√°latot.
    
    Usage:
        graph = create_agent_graph(
            llm_client=client,
            model_selector=selector,
            cost_tracker=tracker,
            node_cache=cache1,
            embedding_cache=cache2
        )
    """
    factory = AgentGraphFactory(
        llm_client=llm_client,
        model_selector=model_selector,
        cost_tracker=cost_tracker,
        node_cache=node_cache,
        embedding_cache=embedding_cache
    )
    
    return factory.create_graph()
```

### 5.5 Application-Level DI (main.py)

**F√°jl**: `app/main.py`

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan - ez a ROOT composition.
    
    Itt t√∂rt√©nik az √ñSSZES dependency l√©trehoz√°sa √©s wiring.
    """
    global llm_client, model_selector, cache, agent_graph
    
    # 1. Create basic dependencies
    model_selector = ModelSelector()
    
    # 2. Create LLM client (runtime decision)
    if settings.openai_api_key:
        llm_client = OpenAIClient(api_key=settings.openai_api_key)
    else:
        llm_client = MockLLMClient(latency_ms=100)
    
    # 3. Create caches
    node_cache = MemoryCache(ttl_seconds=3600)
    embedding_cache = MemoryCache(ttl_seconds=86400)
    
    # 4. Create cost tracker
    cost_tracker = CostTracker(model_selector)
    
    # 5. Create agent graph via factory
    agent_graph = create_agent_graph(
        llm_client=llm_client,
        model_selector=model_selector,
        cost_tracker=cost_tracker,
        node_cache=node_cache,
        embedding_cache=embedding_cache
    )
    
    logger.info("‚úÖ All dependencies wired")
    
    yield
    
    # Cleanup
    await node_cache.clear()
    await embedding_cache.clear()
```

**Dependency Graph:**

```
main.py (composition root)
  ‚îú‚îÄ‚îÄ ModelSelector
  ‚îú‚îÄ‚îÄ OpenAIClient / MockLLMClient
  ‚îú‚îÄ‚îÄ MemoryCache (node_cache)
  ‚îú‚îÄ‚îÄ MemoryCache (embedding_cache)
  ‚îú‚îÄ‚îÄ CostTracker
  ‚îî‚îÄ‚îÄ AgentGraphFactory
        ‚îú‚îÄ‚îÄ TriageNode
        ‚îÇ     ‚îú‚îÄ‚îÄ llm_client
        ‚îÇ     ‚îú‚îÄ‚îÄ cost_tracker
        ‚îÇ     ‚îú‚îÄ‚îÄ model_selector
        ‚îÇ     ‚îî‚îÄ‚îÄ node_cache
        ‚îú‚îÄ‚îÄ RetrievalNode
        ‚îÇ     ‚îú‚îÄ‚îÄ llm_client
        ‚îÇ     ‚îú‚îÄ‚îÄ cost_tracker
        ‚îÇ     ‚îú‚îÄ‚îÄ model_selector
        ‚îÇ     ‚îî‚îÄ‚îÄ embedding_cache
        ‚îú‚îÄ‚îÄ ReasoningNode
        ‚îÇ     ‚îú‚îÄ‚îÄ llm_client
        ‚îÇ     ‚îú‚îÄ‚îÄ cost_tracker
        ‚îÇ     ‚îî‚îÄ‚îÄ model_selector
        ‚îî‚îÄ‚îÄ SummaryNode
              ‚îú‚îÄ‚îÄ llm_client
              ‚îú‚îÄ‚îÄ cost_tracker
              ‚îî‚îÄ‚îÄ model_selector
```

### 5.6 Testing with DI

**Test file example:**

```python
import pytest
from app.nodes.triage_node import TriageNode
from app.llm.mock_client import MockLLMClient

@pytest.mark.asyncio
async def test_triage_node_classification():
    """Test triage node with mock dependencies."""
    
    # Create mock dependencies
    mock_llm = MockLLMClient(latency_ms=10)
    mock_cache = MockCache()
    mock_tracker = MockCostTracker()
    mock_selector = MockModelSelector()
    
    # Inject mocks
    node = TriageNode(
        llm_client=mock_llm,  # ‚Üê Mock!
        cost_tracker=mock_tracker,
        model_selector=mock_selector,
        cache=mock_cache
    )
    
    # Test
    state = {"user_input": "What is Docker?"}
    result = await node.execute(state)
    
    # Assert
    assert result["classification"] in ["simple", "retrieval", "complex"]
    assert mock_llm.call_count == 1  # Verify mock was called
```

**El≈ëny√∂k:**
- Gyors tesztek (nincs h√°l√≥zati h√≠v√°s)
- Determinisztikus (mock mindig ugyanazt adja vissza)
- Izol√°ci√≥ (csak egy node-ot tesztel√ºnk)

### 5.7 Gyakorl√≥ Feladatok

**Feladat 1: Custom Service DI**

```python
class MetricsService:
    """
    TODO: Implement√°lj metrics service-t DI-val.
    
    Dependencies:
    - prometheus_client (Protocol)
    - logger (logging.Logger)
    """
    
    def __init__(self, prometheus_client, logger):
        self.prometheus = prometheus_client
        self.logger = logger
    
    def record_llm_call(self, model: str, tokens: int, cost: float):
        """TODO: Record metrics."""
        pass
```

**Feladat 2: Factory with Conditional Dependencies**

```python
class CacheFactory:
    """
    TODO: √çrj factory-t, ami runtime-ban d√∂nti el melyik cache-t haszn√°lja.
    
    Ha REDIS_URL van:
        RedisCache
    K√ºl√∂nben:
        MemoryCache
    """
    
    @staticmethod
    def create(settings: Settings) -> Cache:
        if settings.redis_url:
            return RedisCache(url=settings.redis_url)
        else:
            return MemoryCache(ttl_seconds=settings.cache_ttl)
```

---

## 6. Pydantic √©s Adatvalid√°ci√≥

### 6.1 Mi az a Pydantic?

**Pydantic** = adatvalid√°ci√≥s library Python-hoz, type hints alapj√°n.

**El≈ëny√∂k:**
- Automatikus valid√°ci√≥
- Type conversion
- JSON serialization/deserialization
- IDE support
- FastAPI integr√°ci√≥val

### 6.2 BaseModel - Alapok

**F√°jl**: `app/main.py`

```python
from pydantic import BaseModel, Field
from typing import Optional

class RunRequest(BaseModel):
    """
    Request model a /run endpoint-hoz.
    
    Pydantic automatikusan:
    - Valid√°lja a t√≠pusokat
    - Konvert√°l (str -> int, stb.)
    - Hib√°t dob rossz adat eset√©n
    """
    user_input: str = Field(..., description="User query")
    scenario: Optional[str] = Field(None, description="Optional scenario hint")

# Haszn√°lat:
request_data = {
    "user_input": "What is Docker?",
    "scenario": "simple"
}

request = RunRequest(**request_data)  # ‚úÖ Valid√°ci√≥ sikeres
print(request.user_input)  # "What is Docker?"

# Hib√°s adat:
bad_data = {"user_input": 123}  # user_input nem string!
request = RunRequest(**bad_data)  # ‚ùå ValidationError!
```

**Field param√©terek:**
- `...` = k√∂telez≈ë mez≈ë
- `None` = default √©rt√©k
- `description` = OpenAPI dokument√°ci√≥hoz
- `min_length`, `max_length` = valid√°ci√≥s szab√°lyok

### 6.3 Nested Models

**F√°jl**: `app/main.py`

```python
class CostBreakdown(BaseModel):
    """Nested model - cost breakdown."""
    total_input_tokens: int
    total_output_tokens: int
    total_cost_usd: float
    by_node: Dict[str, Dict[str, Any]]
    by_model: Dict[str, Dict[str, Any]]

class RunResponse(BaseModel):
    """
    Response model - tartalmaz nested model-t.
    """
    answer: str
    debug: Dict[str, Any]
    benchmark: Optional[BenchmarkSummary] = None

# JSON ‚Üí Pydantic object:
response_data = {
    "answer": "Docker is a containerization platform",
    "debug": {
        "cost_report": {
            "total_input_tokens": 47,
            "total_output_tokens": 15,
            "total_cost_usd": 0.0015,
            "by_node": {...},
            "by_model": {...}
        }
    }
}

response = RunResponse(**response_data)

# Pydantic object ‚Üí JSON:
json_str = response.model_dump_json()
```

### 6.4 Field Validators

```python
from pydantic import BaseModel, Field, field_validator

class QueryRequest(BaseModel):
    """Request with custom validation."""
    
    user_input: str = Field(..., min_length=1, max_length=10000)
    max_tokens: int = Field(100, ge=1, le=4000)  # ge=greater or equal
    temperature: float = Field(0.7, ge=0.0, le=2.0)
    
    @field_validator("user_input")
    @classmethod
    def validate_not_empty(cls, v: str) -> str:
        """Custom validator - nem lehet csak whitespace."""
        if not v.strip():
            raise ValueError("Input cannot be empty or whitespace only")
        return v.strip()
    
    @field_validator("temperature")
    @classmethod
    def validate_temperature(cls, v: float) -> float:
        """Custom validator - figyelmeztet√©s magas √©rt√©kn√©l."""
        if v > 1.0:
            import warnings
            warnings.warn(f"Temperature {v} is unusually high")
        return v

# Haszn√°lat:
request = QueryRequest(
    user_input="  What is Python?  ",  # Trimmed automatically
    max_tokens=500,
    temperature=0.8
)
print(request.user_input)  # "What is Python?" (trimmed)

# Hib√°s:
bad_request = QueryRequest(user_input="   ")  # ‚ùå ValidationError!
```

### 6.5 Settings Management

**F√°jl**: `app/config.py`

```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """
    Application settings - environment v√°ltoz√≥kb√≥l.
    
    Pydantic automatikusan:
    - Beolvassa az .env f√°jlt
    - Konvert√°lja a t√≠pusokat
    - Valid√°lja az √©rt√©keket
    """
    
    # API settings
    openai_api_key: Optional[str] = None
    
    # Server settings
    host: str = "0.0.0.0"
    port: int = 8000
    
    # Model settings
    model_cheap: str = "gpt-3.5-turbo"
    model_medium: str = "gpt-4-turbo"
    model_expensive: str = "gpt-4"
    
    # Cache settings
    cache_ttl_seconds: int = 3600
    cache_max_size: int = 1000
    
    class Config:
        env_file = ".env"  # Load from .env file
        case_sensitive = False  # Environment variables case-insensitive

# Haszn√°lat:
settings = Settings()  # Automatikusan beolvassa az .env-t

print(settings.openai_api_key)  # Vagy None, vagy az .env-b≈ël
print(settings.port)  # 8000 (default) vagy .env-b≈ël
```

**.env f√°jl:**

```bash
# OpenAI
OPENAI_API_KEY=sk-...

# Server
PORT=8080

# Models
MODEL_CHEAP=gpt-3.5-turbo
MODEL_EXPENSIVE=gpt-4

# Cache
CACHE_TTL_SECONDS=7200
CACHE_MAX_SIZE=5000
```

**El≈ëny√∂k:**
- **Type safety**: Minden setting t√≠pusozott
- **Validation**: Hib√°s config ‚Üí startup error
- **Defaults**: Sensible defaults, de fel√ºl√≠rhat√≥
- **Documentation**: Self-documenting

### 6.6 FastAPI Integration

**Automatikus valid√°ci√≥ endpoint-okban:**

```python
from fastapi import FastAPI, HTTPException

app = FastAPI()

@app.post("/run", response_model=RunResponse)
async def run_agent(request: RunRequest):
    """
    FastAPI + Pydantic magic:
    
    1. Request validation:
       - JSON ‚Üí RunRequest object
       - Type checking
       - Field validation
       
    2. Response validation:
       - Return value ‚Üí RunResponse
       - Type checking
       - JSON serialization
    
    3. OpenAPI docs:
       - Automatic schema generation
       - /docs endpoint
    """
    # request m√°r valid√°lt RunRequest object!
    print(request.user_input)  # Type-safe
    
    result = await process(request)
    
    # result MUST be RunResponse, vagy ValidationError!
    return result
```

**Ha rossz adat j√∂n:**

```bash
curl -X POST http://localhost:8000/run \
  -H "Content-Type: application/json" \
  -d '{"user_input": 123}'  # ‚ùå user_input nem string!

# Response:
{
  "detail": [
    {
      "loc": ["body", "user_input"],
      "msg": "str type expected",
      "type": "type_error.str"
    }
  ]
}
```

### 6.7 JSON Schema Generation

```python
from pydantic import BaseModel

class AgentMetadata(BaseModel):
    """Agent execution metadata."""
    nodes_executed: List[str]
    total_cost_usd: float
    cache_hit_ratio: float
    latency_seconds: float

# JSON schema gener√°l√°s:
schema = AgentMetadata.model_json_schema()

print(schema)
# {
#   "title": "AgentMetadata",
#   "type": "object",
#   "properties": {
#     "nodes_executed": {
#       "title": "Nodes Executed",
#       "type": "array",
#       "items": {"type": "string"}
#     },
#     "total_cost_usd": {
#       "title": "Total Cost Usd",
#       "type": "number"
#     },
#     ...
#   },
#   "required": ["nodes_executed", "total_cost_usd", ...]
# }
```

**Haszn√°lat:**
- OpenAPI dokument√°ci√≥
- JSON Schema valid√°torok
- Frontend type generation (TypeScript)

### 6.8 Gyakorl√≥ Feladatok

**Feladat 1: Model with Validation**

```python
from pydantic import BaseModel, Field, field_validator

class AgentConfig(BaseModel):
    """
    TODO: Hozz l√©tre config modelt valid√°ci√≥val.
    
    Mez≈ëk:
    - max_retries: int (1-10 k√∂z√∂tt)
    - timeout_seconds: float (>0)
    - model_tier: Literal["cheap", "medium", "expensive"]
    - enable_cache: bool
    """
    pass
```

**Feladat 2: Nested Response Model**

```python
class NodeExecutionResult(BaseModel):
    """TODO: Node fut√°s eredm√©nye."""
    node_name: str
    latency_seconds: float
    input_tokens: int
    output_tokens: int
    cost_usd: float

class AgentResponse(BaseModel):
    """
    TODO: Teljes agent response nested model-lel.
    
    Mez≈ëk:
    - answer: str
    - node_results: List[NodeExecutionResult]
    - total_cost_usd: float
    """
    pass
```

---

## 7. FastAPI √©s REST API-k

### 7.1 FastAPI Alapok

**FastAPI** = modern, gyors web framework Python-hoz.

**El≈ëny√∂k AI √°gensekn√©l:**
- **Async support**: Nagy teljes√≠tm√©ny
- **Type hints**: Pydantic integr√°ci√≥
- **OpenAPI docs**: Automatikus dokument√°ci√≥
- **Dependency injection**: Built-in DI rendszer

### 7.2 Endpoint Definition

**F√°jl**: `app/main.py`

```python
from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel

app = FastAPI(
    title="AI Agent Cost Optimization Demo",
    description="Educational LangGraph demo",
    version="1.0.0"
)

@app.post("/run", response_model=RunResponse)
async def run_agent(
    request: RunRequest,
    repeat: Optional[int] = Query(None, ge=1, le=1000)
):
    """
    Run the agent workflow.
    
    Args:
        request: RunRequest body (JSON)
        repeat: Optional query param for benchmark mode
        
    Returns:
        RunResponse with answer and debug info
    """
    if repeat and repeat > 1:
        return await _run_benchmark(request, repeat)
    else:
        return await _run_single(request)
```

**URL patterns:**
- `POST /run` - single execution
- `POST /run?repeat=10` - benchmark mode

### 7.3 Request/Response Models

```python
class RunRequest(BaseModel):
    """Request body."""
    user_input: str = Field(..., description="User query")
    scenario: Optional[str] = Field(None, description="Scenario hint")

class RunResponse(BaseModel):
    """Response body."""
    answer: str
    debug: Dict[str, Any]
    benchmark: Optional[BenchmarkSummary] = None

# FastAPI automatikusan:
# 1. Valid√°lja a request body-t
# 2. Deserializ√°lja JSON ‚Üí RunRequest
# 3. Valid√°lja a response-t
# 4. Serializ√°lja RunResponse ‚Üí JSON
```

### 7.4 Query Parameters

```python
@app.get("/metrics")
async def get_metrics(
    node: Optional[str] = Query(None, description="Filter by node name"),
    time_range: int = Query(3600, ge=60, le=86400, description="Time range in seconds")
):
    """
    Get metrics with query parameters.
    
    URL: /metrics?node=triage&time_range=7200
    
    Query parameters:
    - node: Optional filter
    - time_range: Required, default 3600, range 60-86400
    """
    metrics = fetch_metrics(node=node, time_range=time_range)
    return metrics
```

### 7.5 Path Parameters

```python
@app.get("/agent/{agent_id}/status")
async def get_agent_status(agent_id: str):
    """
    Path parameter example.
    
    URL: /agent/abc123/status
    agent_id = "abc123"
    """
    status = get_status(agent_id)
    
    if not status:
        raise HTTPException(status_code=404, detail="Agent not found")
    
    return status
```

### 7.6 Error Handling

```python
from fastapi import HTTPException

@app.post("/run")
async def run_agent(request: RunRequest):
    """Proper error handling."""
    
    try:
        result = await agent_graph.ainvoke(state)
        return result
        
    except ValueError as e:
        # Client error - bad input
        raise HTTPException(
            status_code=400,
            detail=f"Invalid input: {str(e)}"
        )
    
    except TimeoutError:
        # Server error - timeout
        raise HTTPException(
            status_code=504,
            detail="Request timeout"
        )
    
    except Exception as e:
        # Unknown error
        logger.error(f"Unexpected error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Internal server error"
        )
```

### 7.7 Middleware

**F√°jl**: `app/observability/middleware.py`

```python
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
import time

class MetricsMiddleware(BaseHTTPMiddleware):
    """
    Middleware = k√∂ztes r√©teg minden request-n√©l.
    
    Haszn√°lat:
    - Metrics r√∂gz√≠t√©s
    - Logging
    - Authentication
    - Rate limiting
    """
    
    async def dispatch(self, request: Request, call_next):
        """
        Called for EVERY request.
        
        Args:
            request: Incoming request
            call_next: Next middleware/endpoint
        """
        # === BEFORE REQUEST ===
        start_time = time.time()
        path = request.url.path
        method = request.method
        
        logger.info(f"{method} {path} - started")
        
        # === PROCESS REQUEST ===
        try:
            response = await call_next(request)
            
            # === AFTER REQUEST (success) ===
            latency = time.time() - start_time
            status = response.status_code
            
            # Record metrics
            http_requests_total.labels(
                path=path,
                method=method,
                status=status
            ).inc()
            
            http_request_latency_seconds.labels(
                path=path,
                method=method
            ).observe(latency)
            
            logger.info(f"{method} {path} - {status} - {latency:.3f}s")
            
            return response
            
        except Exception as e:
            # === AFTER REQUEST (error) ===
            latency = time.time() - start_time
            
            logger.error(f"{method} {path} - ERROR: {e}")
            
            http_requests_total.labels(
                path=path,
                method=method,
                status=500
            ).inc()
            
            raise

# Register middleware:
app.add_middleware(MetricsMiddleware)
```

### 7.8 Lifespan Events

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifecycle hooks.
    
    Startup:
    - DB connections
    - Cache init
    - Load models
    
    Shutdown:
    - Close connections
    - Flush caches
    - Cleanup
    """
    # STARTUP
    logger.info("üöÄ Starting...")
    
    global db, cache
    db = await connect_database()
    cache = MemoryCache()
    
    logger.info("‚úÖ Ready")
    
    yield  # App runs
    
    # SHUTDOWN
    logger.info("üõë Shutting down...")
    
    await db.close()
    await cache.clear()
    
    logger.info("‚úÖ Cleanup done")

app = FastAPI(lifespan=lifespan)
```

### 7.9 Gyakorl√≥ Feladatok

**Feladat 1: CRUD Endpoints**

```python
@app.post("/agents", response_model=Agent)
async def create_agent(agent: AgentCreate):
    """TODO: Create new agent."""
    pass

@app.get("/agents/{agent_id}", response_model=Agent)
async def get_agent(agent_id: str):
    """TODO: Get agent by ID."""
    pass

@app.put("/agents/{agent_id}", response_model=Agent)
async def update_agent(agent_id: str, agent: AgentUpdate):
    """TODO: Update agent."""
    pass

@app.delete("/agents/{agent_id}")
async def delete_agent(agent_id: str):
    """TODO: Delete agent."""
    pass
```

**Feladat 2: Rate Limiting Middleware**

```python
class RateLimitMiddleware(BaseHTTPMiddleware):
    """
    TODO: Implement√°lj rate limiting middleware-t.
    
    - Max 100 req/min per IP
    - 429 status ha t√∫ll√©p√©s
    - X-RateLimit-* headerek
    """
    pass
```

---

## 8. LangGraph √©s Workflow Orchestration

### 8.1 Mi az a LangGraph?

**LangGraph** = workflow orchestration framework AI √°gensekhez.

**Kulcs fogalmak:**
- **State**: Shared state az √∂sszes node k√∂z√∂tt
- **Node**: Workflow l√©p√©s (f√ºggv√©ny)
- **Edge**: Kapcsolat node-ok k√∂z√∂tt
- **Conditional Edge**: Dinamikus routing

### 8.2 StateGraph Alapok

**F√°jl**: `app/graph/state.py`

```python
from typing_extensions import TypedDict
from typing import List, Dict, Optional

class AgentState(TypedDict, total=False):
    """
    Shared state minden node sz√°m√°ra.
    
    total=False = minden mez≈ë optional
    """
    user_input: str
    classification: Optional[str]
    retrieved_docs: List[str]
    reasoning_output: Optional[str]
    final_answer: Optional[str]
    nodes_executed: List[str]

# Node function signature:
async def node_function(state: AgentState) -> Dict:
    """
    Node function:
    - Kapja a state-et
    - Visszaad dictionary-t (state update)
    - LangGraph merge-eli a state-be
    """
    return {"classification": "simple"}
```

### 8.3 Graph Building

**F√°jl**: `app/graph/agent_graph.py`

```python
from langgraph.graph import StateGraph, END

def create_graph():
    """Build LangGraph workflow."""
    
    # 1. Create graph
    workflow = StateGraph(AgentState)
    
    # 2. Add nodes
    workflow.add_node("triage", triage_node.execute)
    workflow.add_node("retrieval", retrieval_node.execute)
    workflow.add_node("reasoning", reasoning_node.execute)
    workflow.add_node("summary", summary_node.execute)
    
    # 3. Set entry point
    workflow.set_entry_point("triage")
    
    # 4. Add edges
    workflow.add_edge("reasoning", "summary")
    workflow.add_edge("summary", END)
    
    # 5. Compile
    app = workflow.compile()
    
    return app
```

### 8.4 Conditional Routing

```python
from typing import Literal

def route_after_triage(state: AgentState) -> Literal["retrieval", "summary"]:
    """
    Routing function - d√∂nti el a k√∂vetkez≈ë node-ot.
    
    Returns:
        Node name (must match Literal types)
    """
    classification = state.get("classification")
    
    if classification == "simple":
        return "summary"  # Skip retrieval
    else:
        return "retrieval"  # Need docs

# Add conditional edge:
workflow.add_conditional_edges(
    "triage",  # From node
    route_after_triage,  # Routing function
    {
        "retrieval": "retrieval",  # Mapping
        "summary": "summary"
    }
)
```

### 8.5 Node Implementation Pattern

```python
class TriageNode:
    """
    Node class pattern.
    
    Best practice:
    - Dependency injection via constructor
    - execute() method returns state update
    - Logging and metrics
    """
    
    def __init__(self, llm_client: LLMClient, cache: Cache):
        self.llm_client = llm_client
        self.cache = cache
    
    async def execute(self, state: AgentState) -> Dict:
        """
        Execute node logic.
        
        Args:
            state: Current workflow state
            
        Returns:
            Dictionary with state updates
        """
        # 1. Extract from state
        user_input = state["user_input"]
        
        # 2. Process
        classification = await self._classify(user_input)
        
        # 3. Update state
        nodes_executed = state.get("nodes_executed", [])
        nodes_executed.append("triage")
        
        # 4. Return updates
        return {
            "classification": classification,
            "nodes_executed": nodes_executed
        }
```

### 8.6 Execution

```python
# Create graph
graph = create_agent_graph(...)

# Initial state
initial_state: AgentState = {
    "user_input": "What is Docker?",
    "nodes_executed": [],
    "retrieved_docs": []
}

# Execute workflow
final_state = await graph.ainvoke(initial_state)

# Access results
print(final_state["final_answer"])
print(final_state["nodes_executed"])  # ["triage", "summary"]
```

### 8.7 Gyakorl√≥ Feladatok

**Feladat 1: Simple Workflow**

```python
def create_simple_workflow():
    """
    TODO: Hozz l√©tre egyszer≈± workflow-t:
    
    START ‚Üí validate ‚Üí process ‚Üí format ‚Üí END
    
    State:
    - input: str
    - is_valid: bool
    - result: Optional[str]
    - formatted: Optional[str]
    """
    pass
```

**Feladat 2: Conditional Workflow**

```python
def create_conditional_workflow():
    """
    TODO: Workflow conditional routing-gal:
    
    START ‚Üí check ‚Üí (success ‚Üí process ‚Üí END)
                  ‚Üí (failure ‚Üí retry ‚Üí check)
    
    Max 3 retry ut√°n END
    """
    pass
```

---

## 9. Observability √©s Metrics

### 9.1 Prometheus Metrics

**F√°jl**: `app/observability/metrics.py`

```python
from prometheus_client import Counter, Histogram

# Counter - n√∂vekv≈ë sz√°m
llm_inference_count_total = Counter(
    'llm_inference_count_total',
    'Total LLM inference calls',
    ['model', 'node', 'status']  # Labels
)

# Histogram - distribution
llm_inference_latency_seconds = Histogram(
    'llm_inference_latency_seconds',
    'LLM inference latency',
    ['model', 'node'],
    buckets=[0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
)
```

**Haszn√°lat:**

```python
# Increment counter
llm_inference_count_total.labels(
    model="gpt-3.5-turbo",
    node="triage",
    status="success"
).inc()

# Record latency
llm_inference_latency_seconds.labels(
    model="gpt-3.5-turbo",
    node="triage"
).observe(0.523)  # 523ms
```

### 9.2 Helper Functions

```python
def record_llm_call(
    model: str,
    node: str,
    latency: float,
    input_tokens: int,
    output_tokens: int,
    cost: float,
    status: str = "success"
):
    """
    Helper function - egyetlen h√≠v√°ssal minden metrika.
    """
    llm_inference_count_total.labels(
        model=model,
        node=node,
        status=status
    ).inc()
    
    llm_inference_latency_seconds.labels(
        model=model,
        node=node
    ).observe(latency)
    
    llm_inference_token_input_total.labels(
        model=model,
        node=node
    ).inc(input_tokens)
    
    llm_inference_token_output_total.labels(
        model=model,
        node=node
    ).inc(output_tokens)
    
    llm_cost_total_usd.labels(
        model=model,
        node=node
    ).inc(cost)
```

### 9.3 Gyakorl√≥ Feladatok

**Feladat 1: Custom Metrics**

```python
# TODO: Defini√°lj metrik√°kat cache monitoring-hoz
cache_size_bytes = Histogram(...)
cache_eviction_total = Counter(...)
```

**Feladat 2: Metrics Middleware**

```python
class MetricsMiddleware:
    """TODO: HTTP metrics middleware."""
    
    async def dispatch(self, request, call_next):
        # Record request metrics
        pass
```

---

## 10. Best Practices AI √Ågensekn√©l

### 10.1 K√∂lts√©goptimaliz√°l√°s

1. **Model tier selection**: Cheap models egyszer≈± feladatokhoz
2. **Prompt minimaliz√°l√°s**: R√∂vid, hat√©kony promptok
3. **Caching**: Node √©s embedding cache
4. **Early exit**: Skip felesleges node-ok
5. **Token limits**: max_tokens be√°ll√≠t√°sa

### 10.2 Teljes√≠tm√©ny

1. **Async everywhere**: I/O-bound m≈±veletek async
2. **Parallel execution**: asyncio.gather()
3. **Connection pooling**: Reuse connections
4. **Caching strategies**: Multi-level cache

### 10.3 Code Quality

1. **Type hints**: Minden f√ºggv√©ny t√≠pusozott
2. **Protocols**: Interface-alap√∫ tervez√©s
3. **Dependency Injection**: Testable code
4. **Error handling**: Explicit exception kezel√©s

### 10.4 Observability

1. **Structured logging**: JSON logs
2. **Metrics**: Prometheus metrics minden kritikus pontn√°l
3. **Tracing**: Request ID v√©gigk√∂vet√©se
4. **Alerting**: Threshold-based alerts

---

## √ñsszefoglal√°s

Ez a kurzus bemutatta a **Python AI √°gensek fejleszt√©s√©hez** sz√ºks√©ges halad√≥ technik√°kat:

1. ‚úÖ **Async/await**: P√°rhuzamos LLM h√≠v√°sok
2. ‚úÖ **Decoratorok**: Context managerek, middleware
3. ‚úÖ **Type hints**: Protocol-ok, TypedDict
4. ‚úÖ **DI pattern**: Testable, flexible k√≥d
5. ‚úÖ **Pydantic**: Automatikus valid√°ci√≥
6. ‚úÖ **FastAPI**: Modern REST API-k
7. ‚úÖ **LangGraph**: Workflow orchestration
8. ‚úÖ **Observability**: Prometheus metrics

**Tov√°bbi tanul√°shoz:**
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)
- [Pydantic Docs](https://docs.pydantic.dev/)
- [Python Async](https://docs.python.org/3/library/asyncio.html)

---

**K√©sz√≠tette**: AI Agent Optimization Course  
**Verzi√≥**: 1.0  
**D√°tum**: 2026. janu√°r 20.  
**Licenc**: MIT - Oktat√°si c√©lokra
