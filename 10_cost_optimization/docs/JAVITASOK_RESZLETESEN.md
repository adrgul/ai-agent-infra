# Jav√≠t√°sok R√©szletesen - Rossz verzi√≥ vs. J√≥ verzi√≥

**K√©sz√ºlt**: 2026. janu√°r 17.  
**C√©l**: R√©szletes technikai √∫tmutat√≥ a k√∂lts√©goptimaliz√°l√°si jav√≠t√°sokhoz

## üìã Tartalomjegyz√©k

1. [√Åttekint√©s](#√°ttekint√©s)
2. [Prompt Optimaliz√°l√°s](#1-prompt-optimaliz√°l√°s)
3. [Dinamikus Modell V√°laszt√°s](#2-dinamikus-modell-v√°laszt√°s)
4. [Gyors√≠t√≥t√°raz√°s Bekapcsol√°sa](#3-gyors√≠t√≥t√°raz√°s-bekapcsol√°sa)
5. [Munkafolyamat Optimaliz√°l√°s](#4-munkafolyamat-optimaliz√°l√°s)
6. [Token K√∂lts√©gvet√©s Korl√°toz√°sa](#5-token-k√∂lts√©gvet√©s-korl√°toz√°sa)
7. [√ñsszes√≠tett Hat√°s](#√∂sszes√≠tett-hat√°s)

---

## √Åttekint√©s

Ez a dokumentum **konkr√©t k√≥d p√©ld√°kkal** mutatja be, hogyan kell √°tdolgozni egy k√∂lts√©ges, ineffekt√≠v AI √°gensrendszert egy production-ready, k√∂lts√©goptimaliz√°lt v√°ltozatt√°.

### Mit tanulsz ebb≈ël a dokumentumb√≥l?

- ‚úÖ Hogyan √≠rj r√∂vid, hat√©kony promptokat
- ‚úÖ Hogyan v√°lassz olcs√≥bb modelleket egyszer≈± feladatokhoz
- ‚úÖ Hogyan implement√°lj node-szint≈± √©s embedding cache-t
- ‚úÖ Hogyan ker√ºld el a felesleges node-ok futtat√°s√°t
- ‚úÖ Hogyan korl√°tozd a token outputot

### Verzi√≥ √ñsszehasonl√≠t√°s

| Metrika | Rossz verzi√≥ | J√≥ verzi√≥ | Javul√°s |
|---------|--------------|-----------|---------|
| **K√∂lts√©g/egyszer≈± lek√©rdez√©s** | $0.025 | $0.0015 | **94% cs√∂kken√©s** |
| **LLM h√≠v√°sok/lek√©rdez√©s** | 4 (mindig) | 2-4 (adapt√≠v) | **50% √°tlag** |
| **Cache tal√°lati ar√°ny** | 0% | 40-60% | **√öj funkci√≥** |
| **p95 latency** | 4-6s | 1-2s | **70% gyorsabb** |
| **Input tokenek (√°tlag)** | 1200 | 180 | **85% cs√∂kken√©s** |
| **Output tokenek (√°tlag)** | 2500 | 250 | **90% cs√∂kken√©s** |

---

## 1. Prompt Optimaliz√°l√°s

### Probl√©ma: Hossz√∫, besz√©des promptok

A rossz verzi√≥ban a promptok t√∫l hossz√∫ak, felesleges magyar√°zatokkal, ami drasztikusan n√∂veli az input token k√∂lts√©geket.

### ‚ùå ROSSZ P√©lda - Triage Prompt

**F√°jl**: `prompts/triage_prompt.txt` (rossz verzi√≥)

```text
Hello! I'm your friendly AI assistant, and I'm here to help you with your question today!

Before I can provide you with the most helpful and accurate response, I need to carefully analyze and classify the type of question you're asking. This is a very important step in our conversation, and I want to make sure I do it right!

Let me explain the classification system I use:

1. SIMPLE questions are straightforward queries that don't require any additional context or deep analysis. These are questions like "What is the capital of France?" or "What's 2+2?" - questions that have clear, direct answers.

2. RETRIEVAL questions are those that require me to look up specific information from a knowledge base or context...

[... tov√°bbi 20 sor magyar√°zat ...]

Here's your question that I need to classify:
{user_input}

After careful consideration and analysis of your question, taking into account all the nuances and details, my classification is:

Classification:
```

**Token sz√°m**: ~350 input token  
**K√∂lts√©g**: 350 √ó $0.0001/1K = $0.000035 **csak a prompt√©rt**

### ‚úÖ J√ì P√©lda - Optimaliz√°lt Triage Prompt

**F√°jl**: `prompts/triage_prompt.txt` (j√≥ verzi√≥)

```text
Classify the query type. Output ONE word only.

Types:
- simple: factual, direct answer
- retrieval: requires looking up information
- complex: needs reasoning or analysis

Query: {user_input}

Classification:
```

**Token sz√°m**: ~45 input token  
**K√∂lts√©g**: 45 √ó $0.0001/1K = $0.000045  
**Megtakar√≠t√°s**: **87% kevesebb token**

### Implement√°ci√≥ - Prompt Bet√∂lt√©s

**F√°jl**: `app/nodes/triage_node.py`

```python
def _build_prompt(self, user_input: str) -> str:
    """
    Build minimal classification prompt.
    
    Cost optimization: Very short prompt to minimize input tokens.
    """
    # Load optimized prompt from file
    try:
        with open("prompts/triage_prompt.txt", "r") as f:
            template = f.read()
        return template.replace("{user_input}", user_input)
    except FileNotFoundError:
        # Fallback to inline prompt
        return f"""Classify query type. Output ONE word only.

Types:
- simple: factual, direct answer
- retrieval: requires looking up information
- complex: needs reasoning or analysis

Query: {user_input}

Classification:"""
```

### Reasoning Prompt Optimaliz√°l√°s

**‚ùå ROSSZ** (`prompts/reasoning_prompt.txt`):
```text
Greetings! I am your dedicated expert analyst, and I'm absolutely thrilled to help you work through this complex question today!

Let me introduce myself and explain my approach: I am a highly sophisticated analytical system...

[... 40 sor f√∂l√∂sleges bevezet√©s ...]
```

**‚úÖ J√ì** (`prompts/reasoning_prompt.txt`):
```text
Analyze this complex question using step-by-step reasoning.

Question: {user_input}
{context}
Analysis:
```

**Javul√°s**: 95% token cs√∂kken√©s a reasoning promptban

### Summary Prompt Optimaliz√°l√°s

**‚ùå ROSSZ** (`prompts/summary_prompt.txt`):
```text
Welcome! I'm your friendly AI assistant, and I'm here to help you get the perfect answer to your question!

Thank you so much for your patience as I've been working hard to gather all the information you need...

[... 30 sor k√∂sz√∂netnyilv√°n√≠t√°s √©s magyar√°zat ...]
```

**‚úÖ J√ì** (`prompts/summary_prompt.txt`):
```text
Provide a clear, concise answer.

Question: {user_input}
{retrieval_context}{reasoning_output}
Answer:
```

### üìä Prompt Optimaliz√°l√°s Hat√°sa

| Node | Rossz prompt (tokenek) | J√≥ prompt (tokenek) | Megtakar√≠t√°s |
|------|------------------------|---------------------|--------------|
| Triage | 350 | 45 | **87%** |
| Reasoning | 420 | 25 | **94%** |
| Summary | 380 | 30 | **92%** |
| **√Åtlag** | **383** | **33** | **91%** |

---

## 2. Dinamikus Modell V√°laszt√°s

### Probl√©ma: Minden feladatra a legdr√°g√°bb modell

A rossz verzi√≥ban minden node GPT-4-et haszn√°l, m√©g az egyszer≈± oszt√°lyoz√°shoz is.

### ‚ùå ROSSZ Implement√°ci√≥

**F√°jl**: `app/nodes/triage_node.py` (rossz verzi√≥)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector,
    cache: Cache
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    self.cache = cache
    # ‚ùå BAD PRACTICE: Using expensive model for simple classification task
    self.model_name = model_selector.get_model_name(ModelTier.EXPENSIVE)
```

**Modell**: GPT-4  
**K√∂lts√©g**: $0.01/1K input, $0.03/1K output  
**Probl√©ma**: 10-20x dr√°g√°bb mint kellene

### ‚úÖ J√ì Implement√°ci√≥ - Triage Node

**F√°jl**: `app/nodes/triage_node.py` (j√≥ verzi√≥)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector,
    cache: Cache
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    self.cache = cache
    # ‚úÖ GOOD PRACTICE: Use cheapest model for simple classification
    self.model_name = model_selector.get_model_name(ModelTier.CHEAP)
```

**Modell**: GPT-3.5-turbo  
**K√∂lts√©g**: $0.0001/1K input, $0.0002/1K output  
**Megtakar√≠t√°s**: **100x olcs√≥bb** input tokenekre

### ‚úÖ J√ì Implement√°ci√≥ - Summary Node

**F√°jl**: `app/nodes/summary_node.py` (j√≥ verzi√≥)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    # ‚úÖ GOOD PRACTICE: Use medium model for summary - balance quality and cost
    self.model_name = model_selector.get_model_name(ModelTier.MEDIUM)
```

**Modell**: GPT-4-turbo  
**K√∂lts√©g**: $0.001/1K input, $0.002/1K output  
**El≈ëny**: J√≥ min≈ës√©g, de 10x olcs√≥bb mint GPT-4

### ‚úÖ J√ì Implement√°ci√≥ - Reasoning Node

**F√°jl**: `app/nodes/reasoning_node.py` (v√°ltozatlan)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    # ‚úÖ Expensive model justified for complex reasoning
    self.model_name = model_selector.get_model_name(ModelTier.EXPENSIVE)
```

**Modell**: GPT-4  
**Indokl√°s**: Csak komplex lek√©rdez√©sekn√©l fut, ahol meg√©ri a magasabb min≈ës√©g

### ‚úÖ J√ì Implement√°ci√≥ - Retrieval Node

**F√°jl**: `app/nodes/retrieval_node.py` (j√≥ verzi√≥)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector,
    embedding_cache: Cache
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    self.embedding_cache = embedding_cache
    # ‚úÖ GOOD PRACTICE: Use cheap model for retrieval/embedding tasks
    self.model_name = model_selector.get_model_name(ModelTier.CHEAP)
```

### Model Tier Defin√≠ci√≥k

**F√°jl**: `app/llm/models.py`

```python
from enum import Enum

class ModelTier(str, Enum):
    """Model pricing tiers for cost optimization."""
    CHEAP = "cheap"      # gpt-3.5-turbo: $0.0001/$0.0002
    MEDIUM = "medium"    # gpt-4-turbo: $0.001/$0.002
    EXPENSIVE = "expensive"  # gpt-4: $0.01/$0.03
```

### üìä Modell V√°laszt√°s Hat√°sa

Egyszer≈± lek√©rdez√©s p√©lda: "Mi az 2+2?"

| Verzi√≥ | Triage | Retrieval | Reasoning | Summary | √ñssz K√∂lts√©g |
|--------|--------|-----------|-----------|---------|--------------|
| **Rossz** | GPT-4 | GPT-4 | GPT-4 | GPT-4 | **$0.025** |
| **J√≥** | GPT-3.5 | Kimarad | Kimarad | GPT-4-turbo | **$0.0015** |
| **Javul√°s** | 100x olcs√≥bb | - | - | 10x olcs√≥bb | **94% megtakar√≠t√°s** |

---

## 3. Gyors√≠t√≥t√°raz√°s Bekapcsol√°sa

### Probl√©ma: Gyors√≠t√≥t√°r kikapcsolva

A rossz verzi√≥ban a cache logika megvan, de sz√°nd√©kosan ki van kapcsolva minden node-ban.

### ‚ùå ROSSZ Implement√°ci√≥ - Triage Cache

**F√°jl**: `app/nodes/triage_node.py` (rossz verzi√≥)

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute triage node."""
    logger.info(f"Executing {self.NODE_NAME} node")
    
    async with async_timer() as timer_ctx:
        # Check cache first
        cache_key = generate_cache_key(self.NODE_NAME, state["user_input"])
        
        # ‚ùå BAD PRACTICE: Caching disabled - every request hits the LLM
        cache_lookup_start = time.time()
        cached_result = None  # Force cache miss
        cache_lookup_time = time.time() - cache_lookup_start
        
        if cached_result is not None:
            # This never executes...
            logger.info(f"Cache hit for {self.NODE_NAME}")
            # ...
        else:
            # Cache miss - call LLM
            logger.info(f"Cache miss for {self.NODE_NAME}")
            # ...
            response = await self.llm_client.complete(...)
            
            # ‚ùå BAD PRACTICE: Caching disabled - don't save results
            # await self.cache.set(cache_key, classification)
```

**Probl√©ma**: 
- `cached_result = None` - mindig cache miss
- `await self.cache.set(...)` - ki van kommentezve
- Minden azonos lek√©rdez√©s √∫jra h√≠vja az LLM-et

### ‚úÖ J√ì Implement√°ci√≥ - Triage Cache Bekapcsolva

**F√°jl**: `app/nodes/triage_node.py` (j√≥ verzi√≥)

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute triage node."""
    logger.info(f"Executing {self.NODE_NAME} node")
    
    async with async_timer() as timer_ctx:
        # Check cache first
        cache_key = generate_cache_key(self.NODE_NAME, state["user_input"])
        
        # ‚úÖ GOOD PRACTICE: Enable node-level caching for triage
        cache_lookup_start = time.time()
        cached_result = await self.cache.get(cache_key)  # ‚Üê Val√≥di cache lookup
        cache_lookup_time = time.time() - cache_lookup_start
        
        if cached_result is not None:
            # Cache hit - skip LLM call entirely!
            logger.info(f"Cache hit for {self.NODE_NAME}")
            metrics.record_cache_lookup(
                self.CACHE_NAME,
                self.NODE_NAME,
                hit=True,
                latency=cache_lookup_time
            )
            
            classification = cached_result
        else:
            # Cache miss - call LLM
            logger.info(f"Cache miss for {self.NODE_NAME}")
            metrics.record_cache_lookup(
                self.CACHE_NAME,
                self.NODE_NAME,
                hit=False,
                latency=cache_lookup_time
            )
            
            # Load prompt and call LLM
            prompt = self._build_prompt(state["user_input"])
            response = await self.llm_client.complete(
                prompt=prompt,
                model=self.model_name,
                max_tokens=10,
                temperature=0.0
            )
            
            classification = response.content.strip().lower()
            # Normalize...
            
            # Track cost
            self.cost_tracker.track_usage(...)
            metrics.record_llm_call(...)
            
            # ‚úÖ GOOD PRACTICE: Cache triage results for repeated queries
            await self.cache.set(cache_key, classification)  # ‚Üê Ment√©s cache-be
```

**V√°ltoztat√°sok**:
1. `cached_result = await self.cache.get(cache_key)` - val√≥di lookup
2. `await self.cache.set(cache_key, classification)` - ment√©s enged√©lyezve
3. Cache hit eset√©n: **0 LLM h√≠v√°s = 0 k√∂lts√©g**

### ‚úÖ J√ì Implement√°ci√≥ - Embedding Cache

**F√°jl**: `app/nodes/retrieval_node.py` (j√≥ verzi√≥)

```python
async def _get_embedding(self, text: str) -> str:
    """
    Get embedding for text (simulated with caching).
    
    In production, this would call an embedding model.
    Cache prevents recomputing embeddings for the same text.
    """
    cache_key = generate_cache_key(self.CACHE_NAME, text)
    
    # ‚úÖ GOOD PRACTICE: Enable embedding cache to avoid recomputation
    cache_lookup_start = time.time()
    cached_embedding = await self.embedding_cache.get(cache_key)  # ‚Üê Lookup
    cache_lookup_time = time.time() - cache_lookup_start
    
    if cached_embedding is not None:
        logger.info(f"Embedding cache hit")
        metrics.record_cache_lookup(
            self.CACHE_NAME,
            self.NODE_NAME,
            hit=True,
            latency=cache_lookup_time
        )
        return cached_embedding
    
    # Cache miss - compute embedding (simulated)
    logger.info(f"Embedding cache miss")
    metrics.record_cache_lookup(
        self.CACHE_NAME,
        self.NODE_NAME,
        hit=False,
        latency=cache_lookup_time
    )
    
    # Simulate embedding as deterministic hash
    embedding = hashlib.sha256(text.encode()).hexdigest()
    
    # ‚úÖ GOOD PRACTICE: Cache embeddings for reuse
    await self.embedding_cache.set(cache_key, embedding)  # ‚Üê Ment√©s
    
    return embedding
```

### Cache Kulcs Gener√°l√°s

**F√°jl**: `app/cache/keys.py`

```python
import hashlib

def generate_cache_key(prefix: str, content: str) -> str:
    """
    Generate deterministic cache key.
    
    Args:
        prefix: Cache namespace (e.g., "triage", "embedding")
        content: Content to hash (e.g., user input)
    
    Returns:
        Deterministic cache key
    """
    content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
    return f"{prefix}:{content_hash}"
```

**P√©lda**:
- Input: `generate_cache_key("triage", "What is Docker?")`
- Output: `"triage:a3f5c8b2e9d1f4a7"`

### üìä Cache Hat√°sa

P√©lda: Ugyanaz a lek√©rdez√©s 20x (benchmark mode)

| Fut√°s | Cache √Ållapot | LLM H√≠v√°s | K√∂lts√©g | Latency |
|-------|---------------|-----------|---------|---------|
| 1. | Miss | ‚úÖ Igen | $0.0015 | 1.2s |
| 2-20. | Hit | ‚ùå Nem | $0.0000 | 0.05s |
| **√ñssz** | 5% miss, 95% hit | 1 h√≠v√°s | **$0.0015** | ~0.1s √°tlag |

**Rossz verzi√≥ ugyanerre**: 20 √ó $0.025 = **$0.50** (333x dr√°g√°bb!)

---

## 4. Munkafolyamat Optimaliz√°l√°s

### Probl√©ma: Minden node fut minden lek√©rdez√©sn√©l

A rossz verzi√≥ban a routing logika ignor√°lja a triage eredm√©nyt √©s minden node-ot futtat.

### ‚ùå ROSSZ Implement√°ci√≥ - Agent Graph

**F√°jl**: `app/graph/agent_graph.py` (rossz verzi√≥)

```python
def route_after_triage(state: AgentState) -> Literal["retrieval", "reasoning", "summary"]:
    """
    ‚ùå BAD PRACTICE: Ignoring classification - always go to retrieval.
    This ensures ALL nodes run for EVERY request, regardless of actual need.
    """
    classification = state.get("classification")
    logger.info(f"Routing decision (ignored): {classification} - ALWAYS routing to retrieval")
    
    # ‚ùå BAD PRACTICE: Always route to retrieval to ensure all nodes execute
    return "retrieval"

workflow.add_conditional_edges(
    "triage",
    route_after_triage,
    {
        "retrieval": "retrieval",
        "reasoning": "retrieval",  # ‚ùå BAD PRACTICE: Changed to always go to retrieval
        "summary": "retrieval"     # ‚ùå BAD PRACTICE: Changed to always go to retrieval
    }
)

# ‚ùå BAD PRACTICE: Chain all nodes together - retrieval ‚Üí reasoning ‚Üí summary
# This ensures EVERY node runs for EVERY request
workflow.add_edge("retrieval", "reasoning")
workflow.add_edge("reasoning", "summary")
```

**Probl√©ma**:
- "What is 2+2?" ‚Üí triage, retrieval, reasoning, summary (4 node)
- "Hello" ‚Üí triage, retrieval, reasoning, summary (4 node)
- √ñsszes lek√©rdez√©s **mindig 4 node-ot** futtat

### ‚úÖ J√ì Implement√°ci√≥ - Intelligens Routing

**F√°jl**: `app/graph/agent_graph.py` (j√≥ verzi√≥)

```python
def route_after_triage(state: AgentState) -> Literal["retrieval", "reasoning", "summary"]:
    """
    ‚úÖ GOOD PRACTICE: Intelligent routing based on classification.
    
    This workflow optimization dramatically reduces costs:
    - simple: skip retrieval and reasoning, go straight to summary
    - retrieval: do retrieval, skip reasoning, then summary
    - complex: do retrieval and reasoning, then summary
    
    Graph-level caching opportunity:
    LangGraph supports graph-level persistence/checkpointing which could
    cache entire workflow executions. This would be configured via
    MemorySaver or SqliteSaver when compiling the graph.
    Example: app = workflow.compile(checkpointer=MemorySaver())
    """
    classification = state.get("classification")
    logger.info(f"Routing based on classification: {classification}")
    
    # ‚úÖ GOOD PRACTICE: Route intelligently to skip unnecessary nodes
    if classification == "simple":
        # Simple queries: skip all intermediate steps
        return "summary"
    elif classification == "retrieval":
        # Retrieval queries: get context, then summarize
        return "retrieval"
    else:  # complex
        # Complex queries: full pipeline with retrieval and reasoning
        return "retrieval"

workflow.add_conditional_edges(
    "triage",
    route_after_triage,
    {
        "retrieval": "retrieval",
        "reasoning": "retrieval",
        "summary": "summary"  # ‚úÖ Direct path for simple queries
    }
)

# ‚úÖ GOOD PRACTICE: Conditional routing after retrieval
def route_after_retrieval(state: AgentState) -> Literal["reasoning", "summary"]:
    """
    Route to reasoning only for complex queries, otherwise summarize.
    """
    classification = state.get("classification")
    if classification == "complex":
        return "reasoning"
    return "summary"

workflow.add_conditional_edges(
    "retrieval",
    route_after_retrieval,
    {
        "reasoning": "reasoning",
        "summary": "summary"  # ‚úÖ Skip reasoning for retrieval-only queries
    }
)

# Reasoning always goes to summary
workflow.add_edge("reasoning", "summary")
```

### ‚úÖ J√ì Implement√°ci√≥ - Node-szint≈± Early Exit

**F√°jl**: `app/nodes/reasoning_node.py` (j√≥ verzi√≥)

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute reasoning node."""
    logger.info(f"Executing {self.NODE_NAME} node")
    
    # ‚úÖ GOOD PRACTICE: Only run expensive reasoning for complex queries
    if state.get("classification") != "complex":
        logger.info("Skipping reasoning - not a complex query")
        return {
            "nodes_executed": state.get("nodes_executed", []) + [f"{self.NODE_NAME}_skipped"],
        }
    
    # Continue with expensive reasoning...
    async with async_timer() as timer_ctx:
        prompt = self._build_prompt(state["user_input"], state.get("retrieval_context"))
        
        response = await self.llm_client.complete(
            prompt=prompt,
            model=self.model_name,
            max_tokens=1000,
            temperature=0.3
        )
        # ... rest of implementation
```

**F√°jl**: `app/nodes/retrieval_node.py` (j√≥ verzi√≥)

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute retrieval node."""
    logger.info(f"Executing {self.NODE_NAME} node")
    
    # ‚úÖ GOOD PRACTICE: Only run retrieval when classification indicates it's needed
    if state.get("classification") not in ["retrieval", "complex"]:
        logger.info("Skipping retrieval - not needed for this query type")
        return {
            "nodes_executed": state.get("nodes_executed", []) + [f"{self.NODE_NAME}_skipped"],
        }
    
    # Continue with retrieval...
    async with async_timer() as timer_ctx:
        query_embedding = await self._get_embedding(state["user_input"])
        docs = await self._retrieve_documents(state["user_input"], query_embedding)
        # ... rest of implementation
```

### üìä Routing Hat√°sa

| Lek√©rdez√©s T√≠pus | Rossz Verzi√≥ | J√≥ Verzi√≥ | Node Megtakar√≠t√°s |
|------------------|--------------|-----------|-------------------|
| "What is 2+2?" | triage ‚Üí retrieval ‚Üí reasoning ‚Üí summary (4) | triage ‚Üí summary (2) | **50%** |
| "Find Docker docs" | triage ‚Üí retrieval ‚Üí reasoning ‚Üí summary (4) | triage ‚Üí retrieval ‚Üí summary (3) | **25%** |
| "Design distributed system" | triage ‚Üí retrieval ‚Üí reasoning ‚Üí summary (4) | triage ‚Üí retrieval ‚Üí reasoning ‚Üí summary (4) | **0%** (sz√ºks√©ges) |
| **√Åtlag** | **4 node/lek√©rdez√©s** | **2.5 node/lek√©rdez√©s** | **~40%** |

---

## 5. Token K√∂lts√©gvet√©s Korl√°toz√°sa

### Probl√©ma: T√∫l magas max_tokens √©rt√©kek

A rossz verzi√≥ban minden node feleslegesen magas `max_tokens` limitet haszn√°l.

### ‚ùå ROSSZ Implement√°ci√≥ - Pazarl√≥ Token Limitek

**F√°jl**: `app/nodes/triage_node.py` (rossz verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=2000,  # ‚ùå Wastefully high for one-word answer
    temperature=0.0
)
```

**Probl√©ma**: Csak egy sz√≥t v√°runk ("simple", "retrieval", "complex"), de 2000 tokent enged√©lyez√ºnk

**F√°jl**: `app/nodes/reasoning_node.py` (rossz verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=3000,  # ‚ùå Wastefully high
    temperature=0.3
)
```

**Probl√©ma**: 3000 token = ~2250 sz√≥, sokkal t√∂bb mint kellene

**F√°jl**: `app/nodes/summary_node.py` (rossz verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=2000,  # ‚ùå Wastefully high for summary
    temperature=0.5
)
```

**Probl√©ma**: Az √∂sszefoglal√≥ r√∂vid kell legyen, 2000 token felesleges

### ‚úÖ J√ì Implement√°ci√≥ - Szigor√∫ Token Limitek

**F√°jl**: `app/nodes/triage_node.py` (j√≥ verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=10,  # ‚úÖ Only need one word
    temperature=0.0  # Deterministic
)
```

**Indokl√°s**: 
- Kimenet: "simple" (1 token), "retrieval" (1 token), "complex" (1 token)
- 10 token: biztons√°gos marg√≥
- **200x kevesebb** mint a rossz verzi√≥

**F√°jl**: `app/nodes/reasoning_node.py` (j√≥ verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=1000,  # ‚úÖ Sufficient for most complex queries
    temperature=0.3  # Lower for more focused reasoning
)
```

**Indokl√°s**:
- 1000 token = ~750 sz√≥
- El√©g a legt√∂bb komplex elemz√©shez
- **3x kevesebb** mint a rossz verzi√≥

**F√°jl**: `app/nodes/summary_node.py` (j√≥ verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=500,  # ‚úÖ Enough for quality summary
    temperature=0.5  # Balanced creativity
)
```

**Indokl√°s**:
- 500 token = ~375 sz√≥
- Elegend≈ë egy j√≥ √∂sszefoglal√≥hoz
- **4x kevesebb** mint a rossz verzi√≥
- K√©nyszer√≠t t√∂m√∂r v√°laszokra

### üìä Token Limit Hat√°sa

GPT-4 output tokenek √°raz√°sa: **$0.03/1K**

| Node | Rossz max_tokens | J√≥ max_tokens | Megtakar√≠t√°s | K√∂lts√©g cs√∂kken√©s |
|------|------------------|---------------|--------------|-------------------|
| Triage | 2000 | 10 | **99.5%** | $0.06 ‚Üí $0.0003 |
| Reasoning | 3000 | 1000 | **66%** | $0.09 ‚Üí $0.03 |
| Summary | 2000 | 500 | **75%** | $0.06 ‚Üí $0.015 |

**P√©lda sz√°m√≠t√°s** (complex lek√©rdez√©s, mind a 3 node fut):
- Rossz verzi√≥: 2000 + 3000 + 2000 = 7000 max tokens ‚Üí **$0.21** potenci√°lis k√∂lts√©g
- J√≥ verzi√≥: 10 + 1000 + 500 = 1510 max tokens ‚Üí **$0.045** potenci√°lis k√∂lts√©g
- **Megtakar√≠t√°s: 78%**

### A max_tokens Fontoss√°ga

1. **K√∂lts√©g kontroll**: Output tokenek gyakran dr√°g√°bbak mint input
2. **Latency kontroll**: Kevesebb token = gyorsabb gener√°l√°s
3. **Min≈ës√©g kontroll**: K√©nyszer√≠t t√∂m√∂rs√©gre, jobb v√°laszokat eredm√©nyez
4. **Kisz√°m√≠that√≥s√°g**: Fix fels≈ë limit a k√∂lts√©gekre

---

## √ñsszes√≠tett Hat√°s

### Teljes P√©lda: Egyszer≈± Lek√©rdez√©s

**Lek√©rdez√©s**: "What is 2+2?"

#### ‚ùå Rossz Verzi√≥ V√©grehajt√°s

```
1. TRIAGE NODE
   - Model: GPT-4 ($0.01/$0.03)
   - Prompt: 350 tokens
   - Max tokens: 2000
   - Output: ~5 tokens ("simple")
   - K√∂lts√©g: (350 √ó 0.01 + 5 √ó 0.03) / 1000 = $0.0035 + $0.00015 = $0.00365
   - Cache: Nincs

2. RETRIEVAL NODE (felesleges!)
   - Model: GPT-4
   - Embedding compute + lookup
   - K√∂lts√©g: ~$0.004
   - Cache: Nincs

3. REASONING NODE (felesleges!)
   - Model: GPT-4
   - Prompt: 420 tokens
   - Max tokens: 3000
   - Output: ~800 tokens
   - K√∂lts√©g: (420 √ó 0.01 + 800 √ó 0.03) / 1000 = $0.0042 + $0.024 = $0.0282
   - Cache: Nincs

4. SUMMARY NODE
   - Model: GPT-4
   - Prompt: 380 tokens
   - Max tokens: 2000
   - Output: ~150 tokens
   - K√∂lts√©g: (380 √ó 0.01 + 150 √ó 0.03) / 1000 = $0.0038 + $0.0045 = $0.0083

√ñSSZ K√ñLTS√âG: $0.00365 + $0.004 + $0.0282 + $0.0083 = $0.04415
LATENCY: ~5 seconds
NODES: 4
```

#### ‚úÖ J√≥ Verzi√≥ V√©grehajt√°s (els≈ë fut√°s)

```
1. TRIAGE NODE
   - Model: GPT-3.5-turbo ($0.0001/$0.0002)
   - Prompt: 45 tokens
   - Max tokens: 10
   - Output: 1 token ("simple")
   - K√∂lts√©g: (45 √ó 0.0001 + 1 √ó 0.0002) / 1000 = $0.0000045 + $0.0000002 = $0.0000047
   - Cache: Miss ‚Üí ment√©s

2. RETRIEVAL NODE
   - SKIPPED (routing: simple ‚Üí summary)

3. REASONING NODE
   - SKIPPED (routing: simple ‚Üí summary)

4. SUMMARY NODE
   - Model: GPT-4-turbo ($0.001/$0.002)
   - Prompt: 30 tokens
   - Max tokens: 500
   - Output: ~20 tokens
   - K√∂lts√©g: (30 √ó 0.001 + 20 √ó 0.002) / 1000 = $0.00003 + $0.00004 = $0.00007

√ñSSZ K√ñLTS√âG: $0.0000047 + $0.00007 = $0.0000747
LATENCY: ~1.2 seconds
NODES: 2
```

#### ‚úÖ J√≥ Verzi√≥ V√©grehajt√°s (m√°sodik fut√°s - cache hit)

```
1. TRIAGE NODE
   - Cache HIT! ‚Üí "simple"
   - LLM h√≠v√°s: NINCS
   - K√∂lts√©g: $0.00000
   - Latency: ~5ms

2. RETRIEVAL NODE
   - SKIPPED

3. REASONING NODE
   - SKIPPED

4. SUMMARY NODE
   - Model: GPT-4-turbo
   - K√∂lts√©g: ~$0.00007

√ñSSZ K√ñLTS√âG: $0.00007
LATENCY: ~0.5 seconds
NODES: 2 (1 cached)
```

### √ñsszehasonl√≠t√°s

| Metrika | Rossz | J√≥ (1. fut√°s) | J√≥ (2. fut√°s) |
|---------|-------|---------------|---------------|
| **K√∂lts√©g** | $0.044 | $0.000075 | $0.00007 |
| **Latency** | 5s | 1.2s | 0.5s |
| **LLM h√≠v√°sok** | 4 | 2 | 1 |
| **Node-ok** | 4 | 2 | 2 |
| **Cache haszn√°lat** | 0% | 50% | 50% |

**Megtakar√≠t√°s**:
- Els≈ë fut√°s: **99.8%** k√∂lts√©g cs√∂kken√©s
- M√°sodik fut√°s: **99.84%** k√∂lts√©g cs√∂kken√©s
- Latency javul√°s: **76-90%**

### Sk√°l√°zhat√≥s√°gi Hat√°s

**Havi 100,000 lek√©rdez√©s** (50% egyszer≈±, 30% retrieval, 20% komplex):

| Verzi√≥ | Havi K√∂lts√©g | √âves K√∂lts√©g |
|--------|--------------|--------------|
| Rossz | **$2,200** | **$26,400** |
| J√≥ (cache n√©lk√ºl) | **$150** | **$1,800** |
| J√≥ (40% cache hit) | **$90** | **$1,080** |

**Megtakar√≠t√°s**: **$25,320/√©v** (96% cs√∂kken√©s)

---

## Implement√°l√°si Checklist Hallgat√≥knak

### 1. Prompt Optimaliz√°l√°s ‚úÖ

- [ ] T√∂r√∂ld az √∂sszes felesleges bevezet≈ët √©s magyar√°zatot
- [ ] Haszn√°lj r√∂vid, utas√≠t√°sszer≈± nyelvezetet
- [ ] Csak a sz√ºks√©ges inform√°ci√≥kat add meg
- [ ] Teszteld: minimum 80% token cs√∂kken√©s

**F√°jlok**:
- `prompts/triage_prompt.txt`
- `prompts/reasoning_prompt.txt`
- `prompts/summary_prompt.txt`

### 2. Modell V√°laszt√°s ‚úÖ

- [ ] Triage node: `ModelTier.CHEAP`
- [ ] Retrieval node: `ModelTier.CHEAP`
- [ ] Summary node: `ModelTier.MEDIUM`
- [ ] Reasoning node: `ModelTier.EXPENSIVE` (csak ha sz√ºks√©ges)

**F√°jlok**:
- `app/nodes/triage_node.py` - `__init__` met√≥dus
- `app/nodes/retrieval_node.py` - `__init__` met√≥dus
- `app/nodes/summary_node.py` - `__init__` met√≥dus

### 3. Cache Enged√©lyez√©se ‚úÖ

- [ ] Triage node: `cached_result = await self.cache.get(cache_key)`
- [ ] Triage node: `await self.cache.set(cache_key, classification)`
- [ ] Retrieval node: `cached_embedding = await self.embedding_cache.get(cache_key)`
- [ ] Retrieval node: `await self.embedding_cache.set(cache_key, embedding)`

**F√°jlok**:
- `app/nodes/triage_node.py` - `execute` met√≥dus
- `app/nodes/retrieval_node.py` - `_get_embedding` met√≥dus

### 4. Conditional Routing ‚úÖ

- [ ] Implement√°ld `route_after_triage` intelligens logik√°val
- [ ] Implement√°ld `route_after_retrieval` intelligens logik√°val
- [ ] Add hozz√° early exit logik√°t a reasoning node-hoz
- [ ] Add hozz√° early exit logik√°t a retrieval node-hoz

**F√°jlok**:
- `app/graph/agent_graph.py` - routing f√ºggv√©nyek
- `app/nodes/reasoning_node.py` - `execute` met√≥dus elej√©n
- `app/nodes/retrieval_node.py` - `execute` met√≥dus elej√©n

### 5. Token Limitek ‚úÖ

- [ ] Triage: `max_tokens=10`
- [ ] Reasoning: `max_tokens=1000`
- [ ] Summary: `max_tokens=500`

**F√°jlok**:
- `app/nodes/triage_node.py` - `execute` met√≥dus, `llm_client.complete` h√≠v√°s
- `app/nodes/reasoning_node.py` - `execute` met√≥dus, `llm_client.complete` h√≠v√°s
- `app/nodes/summary_node.py` - `execute` met√≥dus, `llm_client.complete` h√≠v√°s

---

## Tesztel√©s

### Lok√°lis Teszt

```bash
# Ind√≠tsd el a szolg√°ltat√°sokat
docker compose up --build

# Teszt: egyszer≈± lek√©rdez√©s
curl -X POST http://localhost:8000/run \
  -H "Content-Type: application/json" \
  -d '{"user_input": "What is 2+2?"}'

# Elv√°rt eredm√©ny:
# - nodes_executed: ["triage", "summary"]
# - cache_hits: {triage: false} (els≈ë fut√°s)
# - models_used: ["gpt-3.5-turbo", "gpt-4-turbo"]
# - total_cost_usd: ~$0.00008
```

### Cache Teszt

```bash
# Ugyanaz a lek√©rdez√©s 3x
for i in {1..3}; do
  curl -X POST http://localhost:8000/run \
    -H "Content-Type: application/json" \
    -d '{"user_input": "What is Docker?"}'
  echo ""
  sleep 2
done

# Elv√°rt:
# 1. fut√°s: cache_hits: {triage: false}
# 2. fut√°s: cache_hits: {triage: true}  ‚Üê FONTOS!
# 3. fut√°s: cache_hits: {triage: true}
```

### Grafana Metrik√°k

Nyisd meg: http://localhost:3000

Ellen≈ërizd:
- ‚úÖ `llm_inference_count_total{model="gpt-3.5-turbo"}` - n≈ë (triage)
- ‚úÖ `llm_inference_count_total{model="gpt-4-turbo"}` - n≈ë (summary)
- ‚úÖ `llm_inference_count_total{model="gpt-4"}` - NEM n≈ë (egyszer≈± lek√©rdez√©sekn√©l)
- ‚úÖ `cache_hit_total{cache="node_cache"}` - n≈ë a m√°sodik fut√°st√≥l
- ‚úÖ `llm_cost_total_usd` - alacsony marad

---

## Gyakori Hib√°k √©s Megold√°sok

### Hiba 1: Cache nem m≈±k√∂dik

**T√ºnet**: `cache_hits` mindig `false`

**Ok**: Elfelejtett√©l `await` kulcssz√≥t haszn√°lni

```python
# ‚ùå ROSSZ
cached_result = self.cache.get(cache_key)  # Nem await!

# ‚úÖ J√ì
cached_result = await self.cache.get(cache_key)
```

### Hiba 2: Minden node mindig fut

**T√ºnet**: `nodes_executed` mindig 4 node

**Ok**: Nem implement√°ltad a conditional routing-ot

**Megold√°s**: Ellen≈ërizd `app/graph/agent_graph.py` routing logik√°t

### Hiba 3: M√©g mindig dr√°ga

**T√ºnet**: `total_cost_usd` > $0.01 egyszer≈± lek√©rdez√©sn√©l

**Ok**: 
1. Nem cser√©lt a cheap model-re a triage
2. Nem cs√∂kkentetted a max_tokens-t
3. Verbose prompts haszn√°lata

**Megold√°s**: Ellen≈ërizd mind az 5 jav√≠t√°si pontot

### Hiba 4: SyntaxError a promptokban

**T√ºnet**: Prompt f√°jl bet√∂lt√©si hiba

**Ok**: Elfelejtett `"""` a docstring-ben

**Megold√°s**: Ellen≈ërizd az id√©z≈ëjeleket:
```python
def _build_prompt(self, state: AgentState) -> str:
    """
    Build prompt.
    """  # ‚Üê Fontos: 3 id√©z≈ëjel
    # ...
```

---

## K√∂vetkez≈ë L√©p√©sek

1. ‚úÖ Implement√°ld mind az 5 jav√≠t√°st
2. ‚úÖ Teszteld lok√°lisan
3. ‚úÖ Ellen≈ërizd a Grafana metrik√°kat
4. ‚úÖ Dokument√°ld a v√°ltoztat√°sokat
5. ‚úÖ Commit-old a k√≥dot git-be

**Sikeres implement√°ci√≥ jele**:
- 90%+ k√∂lts√©g cs√∂kken√©s
- 40%+ cache hit ratio (m√°sodik fut√°st√≥l)
- 2-3 node √°tlagosan (nem mindig 4)
- Sub-second latency cache hit eset√©n

---

**K√©sz√≠tette**: AI Agent Optimization Course  
**D√°tum**: 2026. janu√°r 17.  
**Verzi√≥**: 1.0  
**Licenc**: MIT - Oktat√°si c√©lokra
