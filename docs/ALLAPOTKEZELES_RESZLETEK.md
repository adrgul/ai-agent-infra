# AI Agent √Ållapotkezel√©s - R√©szletes √ötmutat√≥

## Tartalomjegyz√©k

1. [Bevezet√©s - Mi az Agent √Ållapot?](#1-bevezet√©s---mi-az-agent-√°llapot)
2. [√Ållapot T√≠pusok](#2-√°llapot-t√≠pusok)
3. [√Ållapotkezel√©si M√≥dszerek](#3-√°llapotkezel√©si-m√≥dszerek)
4. [T√°rol√°si Megold√°sok](#4-t√°rol√°si-megold√°sok)
5. [Production K√∂rnyezet - AWS](#5-production-k√∂rnyezet---aws)
6. [K√≥d P√©ld√°k √©s Konfigur√°ci√≥](#6-k√≥d-p√©ld√°k-√©s-konfigur√°ci√≥)
7. [Best Practices](#7-best-practices)

---

## 1. Bevezet√©s - Mi az Agent √Ållapot?

Az AI agent √°llapota **minden olyan adat, amely sz√ºks√©ges a workflow v√©grehajt√°s√°hoz √©s k√∂vet√©s√©hez**. Ez mag√°ban foglalja:

- **User input**: a felhaszn√°l√≥ k√©rd√©se
- **Intermediate results**: k√∂ztes eredm√©nyek (klasszifik√°ci√≥, retrieval, reasoning)
- **Final output**: v√©gs≈ë v√°lasz
- **Metadata**: fut√°si metaadatok (id≈ëz√≠t√©s, k√∂lts√©gek, cache tal√°latok)

### √Ållapot Lifecycle

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  API Call   ‚îÇ  
‚îÇ  /run       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Initial State      ‚îÇ  ‚Üê State l√©trehoz√°s (√ºres mez≈ëkkel)
‚îÇ  Created            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Triage Node        ‚îÇ  ‚Üê State friss√≠t√©s: classification
‚îÇ  Executes           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Retrieval Node     ‚îÇ  ‚Üê State friss√≠t√©s: retrieved_docs
‚îÇ  Executes           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Reasoning Node     ‚îÇ  ‚Üê State friss√≠t√©s: reasoning_output
‚îÇ  Executes           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Summary Node       ‚îÇ  ‚Üê State friss√≠t√©s: final_answer
‚îÇ  Executes           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Final State        ‚îÇ  ‚Üê State visszaad√°sa API v√°laszban
‚îÇ  Returned           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  State Discarded    ‚îÇ  ‚Üê √Ållapot megsemmis√ºl (stateless!)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. √Ållapot T√≠pusok

### 2.1 Workflow √Ållapot (Execution State)

**Defin√≠ci√≥:** Az egyetlen k√©r√©s v√©grehajt√°sa sor√°n √°thalad√≥ adat.

**Lifetime:** Egy API h√≠v√°s id≈ëtartama (~1-10 m√°sodperc)

**Ahol t√°roljuk:** Mem√≥ria (Python objektum)

**P√©lda k√≥d - `app/graph/state.py`:**

```python
from typing import List, Dict, Any, Optional
from typing_extensions import TypedDict


class AgentState(TypedDict, total=False):
    """
    Workflow execution state.
    
    Ez az √°llapot egy k√©r√©s teljes √©letciklusa alatt √©l,
    majd eldob√°sra ker√ºl a v√°lasz visszaad√°sa ut√°n.
    
    MINDEN node l√°tja √©s m√≥dos√≠thatja ezt az √°llapotot.
    """
    
    # ============ INPUT ============
    user_input: str                      # Felhaszn√°l√≥ k√©rd√©se
    scenario: Optional[str]              # Opcion√°lis scenario hint
    
    # ============ NODE OUTPUTS ============
    # Triage node output
    classification: Optional[str]        # "simple" | "retrieval" | "complex"
    
    # Retrieval node output
    retrieved_docs: List[str]            # Visszakeresett dokumentumok
    retrieval_context: Optional[str]     # √ñsszef≈±z√∂tt kontextus
    
    # Reasoning node output
    reasoning_output: Optional[str]      # Reasoning eredm√©ny
    
    # Summary node output
    final_answer: Optional[str]          # V√©gs≈ë v√°lasz
    
    # ============ METADATA ============
    nodes_executed: List[str]            # V√©grehajtott node-ok list√°ja
    models_used: List[str]               # Haszn√°lt LLM modellek
    timings: Dict[str, float]            # Node-onk√©nti fut√°si id≈ëk (mp)
    cache_hits: Dict[str, bool]          # Cache tal√°latok node-onk√©nt
```

**Haszn√°lat a node-okban:**

```python
# app/nodes/triage_node.py - r√©szlet

async def execute(self, state: AgentState) -> Dict:
    """
    Triage node execution.
    
    Args:
        state: Bej√∂v≈ë √°llapot (user_input-tal)
        
    Returns:
        State friss√≠t√©sek (merge-el≈ëdnek a jelenlegi state-be)
    """
    user_input = state["user_input"]  # Olvas√°s
    
    # ... LLM h√≠v√°s, klasszifik√°ci√≥ ...
    
    # State friss√≠t√©s - csak az √∫j mez≈ëket adjuk vissza
    return {
        "classification": classification,
        "nodes_executed": state.get("nodes_executed", []) + ["triage"],
        "models_used": state.get("models_used", []) + [self.model_name],
        "timings": {
            **state.get("timings", {}),
            "triage": execution_time
        },
        "cache_hits": {
            **state.get("cache_hits", {}),
            "triage": cache_hit
        }
    }
```

### 2.2 Cache √Ållapot (Cached Data)

**Defin√≠ci√≥:** El≈ëz≈ë fut√°sok eredm√©nyeinek t√°rol√°sa a gyorsabb v√°laszad√°shoz.

**Lifetime:** Konfigur√°lhat√≥ TTL (pl. 1 √≥ra)

**Ahol t√°roljuk:** Mem√≥ria (MemoryCache) vagy Redis (production-ben)

**P√©lda - `app/cache/memory_cache.py`:**

```python
"""
In-memory cache implementation with TTL support.
"""
import asyncio
import time
from typing import Optional, Any, Dict
from dataclasses import dataclass


@dataclass
class CacheEntry:
    """
    Cache entry with expiration.
    
    Minden cache bejegyz√©s tartalmazza:
    - value: a t√°rolt adat (pl. LLM v√°lasz)
    - expires_at: lej√°rati id≈ëb√©lyeg (Unix timestamp)
    """
    value: Any
    expires_at: float


class MemoryCache:
    """
    TTL-alap√∫ mem√≥ria cache.
    
    √ÅLLAPOT T√çPUS: Cached data
    LIFETIME: cache_ttl_seconds (pl. 3600 mp = 1 √≥ra)
    SCOPE: Alkalmaz√°s szint≈± (minden k√©r√©s megosztja)
    PERSISTENCE: Nincs - √∫jraind√≠t√°skor elv√©sz
    """
    
    def __init__(self, default_ttl_seconds: int = 3600, max_size: int = 1000):
        """
        Args:
            default_ttl_seconds: Alap√©rtelmezett TTL (m√°sodperc)
            max_size: Maximum cache bejegyz√©sek sz√°ma
        """
        self.default_ttl = default_ttl_seconds
        self.max_size = max_size
        self._store: Dict[str, CacheEntry] = {}  # ‚Üê √ÅLLAPOT ITT
        self._lock = asyncio.Lock()
    
    async def get(self, key: str) -> Optional[Any]:
        """
        √ârt√©k lek√©r√©se cache-b≈ël.
        
        Ha lej√°rt vagy nincs -> None
        Ha √©rv√©nyes -> √©rt√©k
        """
        async with self._lock:
            entry = self._store.get(key)
            
            if entry is None:
                return None
            
            # Lej√°rat ellen≈ërz√©s
            if time.time() > entry.expires_at:
                del self._store[key]  # T√∂rl√©s
                return None
            
            return entry.value
    
    async def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None):
        """
        √ârt√©k t√°rol√°sa cache-ben TTL-lel.
        """
        ttl = ttl_seconds if ttl_seconds is not None else self.default_ttl
        expires_at = time.time() + ttl
        
        async with self._lock:
            # LRU eviction ha megtelt
            if len(self._store) >= self.max_size and key not in self._store:
                oldest_key = next(iter(self._store))
                del self._store[oldest_key]
            
            self._store[key] = CacheEntry(value=value, expires_at=expires_at)
```

**Cache haszn√°lat p√©lda:**

```python
# app/nodes/triage_node.py - cache haszn√°lat

from app.cache.keys import generate_cache_key

async def execute(self, state: AgentState) -> Dict:
    """Triage node with caching."""
    
    user_input = state["user_input"]
    
    # Cache kulcs gener√°l√°s (hash alap√∫)
    cache_key = generate_cache_key("triage", user_input)
    
    # 1. Cache lookup
    cached_result = await self.cache.get(cache_key)
    
    if cached_result is not None:
        # Cache HIT - nincs LLM h√≠v√°s!
        logger.info("Cache HIT - returning cached classification")
        return {
            "classification": cached_result,
            "cache_hits": {..., "triage": True}
        }
    
    # 2. Cache MISS - LLM h√≠v√°s sz√ºks√©ges
    logger.info("Cache MISS - calling LLM")
    response = await self.llm_client.complete(prompt, model)
    classification = response.content.strip()
    
    # 3. Cache ment√©s
    await self.cache.set(cache_key, classification)
    
    return {
        "classification": classification,
        "cache_hits": {..., "triage": False}
    }
```

### 2.3 Perzisztens √Ållapot (Conversation History)

**Defin√≠ci√≥:** Hosszabb t√°v√∫ t√°rol√°s (multi-turn conversation, session history).

**Lifetime:** Session lifetime (√≥r√°k/napok) vagy v√©gtelen

**Ahol t√°roljuk:** Database (PostgreSQL, DynamoDB) vagy LangGraph Checkpointer

**FONTOS:** Ebben a projektben **NEM HASZN√ÅLJUK** - minden k√©r√©s stateless!

**Elm√©leti p√©lda (LangGraph checkpointer):**

```python
# NEM HASZN√ÅLT - csak p√©lda

from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.checkpoint.postgres import PostgresSaver

# Memory-based persistence (csak development)
checkpointer = MemorySaver()

# SQLite-based persistence
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

# PostgreSQL-based persistence (production)
checkpointer = PostgresSaver.from_conn_string(
    "postgresql://user:pass@host:5432/db"
)

# Graph compile checkpointer-rel
app = workflow.compile(checkpointer=checkpointer)

# Haszn√°lat thread_id-val (session azonos√≠t√≥)
config = {"configurable": {"thread_id": "user_123_session_456"}}

# Els≈ë k√©r√©s - state ment√©sre ker√ºl
result1 = await app.ainvoke(
    {"user_input": "What is AI?"},
    config=config
)

# M√°sodik k√©r√©s - el≈ëz≈ë state bet√∂lt≈ëdik
result2 = await app.ainvoke(
    {"user_input": "Tell me more"},
    config=config  # Ugyanaz a thread_id!
)
```

**Mi√©rt NEM haszn√°ljuk ebben a projektben?**

1. **Egyszer≈±s√©g**: Minden k√©r√©s f√ºggetlen (stateless API)
2. **K√∂lts√©g optimaliz√°l√°s**: Nem kell database-t fenntartani
3. **Sk√°l√°zhat√≥s√°g**: K√∂nnyebb horizontal scaling
4. **Oktat√°si c√©lok**: F√≥kusz a workflow optimaliz√°l√°son, nem session managementen

---

## 3. √Ållapotkezel√©si M√≥dszerek

### 3.1 LangGraph State Management

**M√≥dszer:** TypedDict-based state dictionary, node-ok k√∂z√∂tti merge

**M≈±k√∂d√©s:**

```python
# 1. Initial state l√©trehoz√°sa
initial_state: AgentState = {
    "user_input": "What is AI?",
    "scenario": None,
    "classification": None,
    "retrieved_docs": [],
    # ... tov√°bbi mez≈ëk None-nal ...
    "nodes_executed": [],
    "timings": {},
    "cache_hits": {}
}

# 2. Graph futtat√°s
final_state = await graph.ainvoke(initial_state)

# 3. Minden node friss√≠ti a state-t (merge)
# Triage node:
state = {**state, "classification": "complex", "nodes_executed": ["triage"]}

# Retrieval node:
state = {**state, "retrieved_docs": [...], "nodes_executed": ["triage", "retrieval"]}

# ... stb
```

**State merge strat√©gia:**

```python
# app/graph/agent_graph.py - LangGraph automatikusan merge-eli

def triage_node(state: AgentState) -> Dict:
    # Return csak az √∫j/m√≥dos√≠tott mez≈ëket
    return {
        "classification": "complex"  # ‚Üê Ez merge-el≈ëdik
    }

def retrieval_node(state: AgentState) -> Dict:
    # state["classification"] m√°r el√©rhet≈ë (triage-b≈ël)
    classification = state["classification"]
    
    # Return √∫j mez≈ëket
    return {
        "retrieved_docs": ["doc1", "doc2"]  # ‚Üê Ez is merge-el≈ëdik
    }
```

**El≈ëny√∂k:**
- ‚úÖ Immut√°bilis pattern (funkcion√°lis programoz√°s)
- ‚úÖ Minden node l√°tja az el≈ëz≈ë eredm√©nyeket
- ‚úÖ T√≠pusbiztos (TypedDict)
- ‚úÖ K√∂nny≈± debugol√°s (l√°that√≥ a state minden l√©p√©sn√©l)

### 3.2 Dependency Injection Pattern

**M√≥dszer:** Cache √©s cost tracker injekt√°l√°sa a node-okba

**P√©lda - `app/graph/agent_graph.py`:**

```python
class AgentGraphFactory:
    """
    Agent graph factory with dependency injection.
    
    Ez a "composition root" - itt k√∂tj√ºk √∂ssze a dependency-ket.
    """
    
    def __init__(
        self,
        llm_client: LLMClient,
        model_selector: ModelSelector,
        cost_tracker: CostTracker,
        node_cache: Cache,           # ‚Üê Cache dependency
        embedding_cache: Cache        # ‚Üê Cache dependency
    ):
        self.llm_client = llm_client
        self.model_selector = model_selector
        self.cost_tracker = cost_tracker
        self.node_cache = node_cache
        self.embedding_cache = embedding_cache
    
    def create_graph(self):
        """Create graph with injected dependencies."""
        
        # Node-ok l√©trehoz√°sa dependency injection-nel
        triage_node = TriageNode(
            llm_client=self.llm_client,
            cost_tracker=self.cost_tracker,
            model_selector=self.model_selector,
            cache=self.node_cache  # ‚Üê Cache injekt√°l√°s
        )
        
        retrieval_node = RetrievalNode(
            llm_client=self.llm_client,
            cost_tracker=self.cost_tracker,
            model_selector=self.model_selector,
            embedding_cache=self.embedding_cache  # ‚Üê Cache injekt√°l√°s
        )
        
        # Graph √©p√≠t√©s
        workflow = StateGraph(AgentState)
        workflow.add_node("triage", triage_node.execute)
        workflow.add_node("retrieval", retrieval_node.execute)
        # ...
        
        return workflow.compile()
```

**El≈ëny√∂k:**
- ‚úÖ K√∂nny≈± mock-ol√°s tesztel√©shez
- ‚úÖ Cache implement√°ci√≥ cser√©lhet≈ë (Memory ‚Üí Redis)
- ‚úÖ Tiszta dependency l√°ncok
- ‚úÖ SOLID elvek betart√°sa

### 3.3 Cache Key Generation

**M√≥dszer:** Determinisztikus cache kulcs gener√°l√°s hash alapj√°n

**P√©lda - `app/cache/keys.py`:**

```python
"""
Cache key generation utilities.
"""
import hashlib
import json
from typing import Any


def generate_cache_key(node_name: str, *args: Any) -> str:
    """
    Generate deterministic cache key.
    
    Args:
        node_name: Node azonos√≠t√≥
        *args: Argumentumok (pl. user_input)
        
    Returns:
        Cache key string (pl. "triage:abc123def456")
    """
    # Serialize argumentumok
    serialized = json.dumps(args, sort_keys=True)
    
    # SHA256 hash
    hash_digest = hashlib.sha256(serialized.encode()).hexdigest()
    
    # Key form√°tum: "node_name:hash"
    return f"{node_name}:{hash_digest[:16]}"


# Haszn√°lat
cache_key = generate_cache_key("triage", "What is AI?")
# Eredm√©ny: "triage:7f3a9b2c1e5d8f0a"

# Ugyanaz az input -> ugyanaz a kulcs
cache_key2 = generate_cache_key("triage", "What is AI?")
assert cache_key == cache_key2  # True!

# M√°s input -> m√°s kulcs
cache_key3 = generate_cache_key("triage", "What is ML?")
assert cache_key != cache_key3  # True!
```

---

## 4. T√°rol√°si Megold√°sok

### 4.1 Memory-based Storage (Jelenlegi Implement√°ci√≥)

**Technol√≥gia:** Python dictionary (`Dict[str, CacheEntry]`)

**Haszn√°lat:**
- Development k√∂rnyezet
- Single-instance deployment
- R√∂vid TTL cache (1 √≥ra)

**Konfigur√°ci√≥ - `app/config.py`:**

```python
class Settings(BaseSettings):
    # Cache konfigur√°ci√≥
    cache_ttl_seconds: int = 3600    # 1 √≥ra
    cache_max_size: int = 1000       # Max 1000 bejegyz√©s
```

**Inicializ√°l√°s - `app/main.py`:**

```python
from app.cache.memory_cache import MemoryCache

# Global cache instances
node_cache: Optional[MemoryCache] = None
embedding_cache: Optional[MemoryCache] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global node_cache, embedding_cache
    
    # Cache inicializ√°l√°s startup-kor
    node_cache = MemoryCache(
        default_ttl_seconds=settings.cache_ttl_seconds,  # 3600s
        max_size=settings.cache_max_size                  # 1000
    )
    
    embedding_cache = MemoryCache(
        default_ttl_seconds=settings.cache_ttl_seconds,
        max_size=settings.cache_max_size
    )
    
    logger.info("Caches initialized (TTL=3600s, max_size=1000)")
    
    yield
    
    # Cleanup shutdown-kor
    await node_cache.clear()
    await embedding_cache.clear()
```

**El≈ëny√∂k:**
- ‚úÖ Egyszer≈± implement√°ci√≥
- ‚úÖ Nincs k√ºls≈ë dependency
- ‚úÖ Gyors (lok√°lis mem√≥ria)

**H√°tr√°nyok:**
- ‚ùå Nem perzisztens (restart = adatveszt√©s)
- ‚ùå Nem osztott (multi-instance eset√©n)
- ‚ùå Mem√≥ria korl√°t

### 4.2 Redis-based Storage (Production Alternat√≠va)

**Technol√≥gia:** Redis in-memory database

**Mikor haszn√°ljuk:**
- Multi-instance deployment
- Shared cache t√∂bb pod k√∂z√∂tt
- Hosszabb TTL (√≥r√°k/napok)

**Implement√°ci√≥ p√©lda - `app/cache/redis_cache.py`:**

```python
"""
Redis cache implementation (NEM IMPLEMENT√ÅLT - csak p√©lda).
"""
import json
from typing import Optional, Any
from redis.asyncio import Redis
from app.cache.interfaces import Cache


class RedisCache(Cache):
    """
    Redis-based cache with TTL support.
    
    √ÅLLAPOT T√çPUS: Cached data
    STORAGE: Redis (external service)
    PERSISTENCE: Redis RDB/AOF
    SCOPE: Shared across all instances
    """
    
    def __init__(self, redis_url: str, default_ttl_seconds: int = 3600):
        """
        Args:
            redis_url: Redis connection URL
            default_ttl_seconds: Default TTL
        """
        self.redis = Redis.from_url(redis_url)
        self.default_ttl = default_ttl_seconds
    
    async def get(self, key: str) -> Optional[Any]:
        """Get from Redis."""
        value = await self.redis.get(key)
        if value is None:
            return None
        return json.loads(value)
    
    async def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None):
        """Set in Redis with TTL."""
        ttl = ttl_seconds if ttl_seconds is not None else self.default_ttl
        serialized = json.dumps(value)
        await self.redis.setex(key, ttl, serialized)
    
    async def delete(self, key: str):
        """Delete from Redis."""
        await self.redis.delete(key)
    
    async def clear(self):
        """Clear all keys (DANGEROUS!)."""
        await self.redis.flushdb()
```

**Docker Compose kieg√©sz√≠t√©s (ha haszn√°ln√°nk):**

```yaml
# docker-compose.yml - Redis hozz√°ad√°sa

services:
  # ... megl√©v≈ë services ...
  
  redis:
    image: redis:7-alpine
    container_name: redis-cache
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - monitoring

volumes:
  # ... megl√©v≈ë volumes ...
  redis-data:
```

**AWS ElastiCache (Production):**

```terraform
# terraform/elasticache.tf - NEM IMPLEMENT√ÅLT (p√©lda)

resource "aws_elasticache_cluster" "redis" {
  cluster_id           = "${var.project_name}-redis"
  engine               = "redis"
  node_type            = "cache.t3.micro"
  num_cache_nodes      = 1
  parameter_group_name = "default.redis7"
  port                 = 6379
  
  subnet_group_name    = aws_elasticache_subnet_group.redis.name
  security_group_ids   = [aws_security_group.redis.id]
  
  tags = {
    Name = "${var.project_name}-redis-cache"
  }
}
```

### 4.3 Database-based Storage (Conversation History)

**Technol√≥gia:** PostgreSQL vagy DynamoDB

**Mikor haszn√°ljuk:**
- Multi-turn conversations
- User session tracking
- Long-term conversation history

**P√©lda s√©ma (PostgreSQL):**

```sql
-- NEM IMPLEMENT√ÅLT - csak p√©lda

CREATE TABLE conversation_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id VARCHAR(255) NOT NULL,
    thread_id VARCHAR(255) NOT NULL UNIQUE,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE conversation_messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES conversation_sessions(id),
    role VARCHAR(50) NOT NULL,  -- 'user' | 'assistant'
    content TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE agent_checkpoints (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    thread_id VARCHAR(255) NOT NULL,
    checkpoint_id VARCHAR(255) NOT NULL,
    state JSONB NOT NULL,  -- Teljes AgentState
    created_at TIMESTAMP DEFAULT NOW(),
    
    UNIQUE(thread_id, checkpoint_id)
);
```

---

## 5. Production K√∂rnyezet - AWS

### 5.1 Jelenlegi Production √Ållapot T√°rol√°s

**AWS ECS Fargate - Stateless Containers**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Application Load Balancer             ‚îÇ
‚îÇ              (ALB - ELB v2)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚îÇ HTTP/HTTPS
             ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ECS Service     ‚îÇ
   ‚îÇ   (Fargate)       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ             ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
  ‚îÇTask 1‚îÇ      ‚îÇTask 2‚îÇ  ‚Üê Horizontal scaling (0-N tasks)
  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò
      ‚îÇ             ‚îÇ
      ‚îÇ  MEMORY     ‚îÇ  MEMORY
      ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  ‚îÇState ‚îÇ   ‚îÇ  ‚îÇState ‚îÇ  ‚Üê Workflow state (per request)
      ‚îÇ  ‚îÇCache ‚îÇ   ‚îÇ  ‚îÇCache ‚îÇ  ‚Üê MemoryCache (TTL=1h)
      ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ             ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
       Data elv√©sz
       restart ut√°n!
```

**√Ållapot t√°rol√°s helye:**

| √Ållapot T√≠pus | T√°rol√°si Hely | Persistence | Shared |
|---------------|---------------|-------------|--------|
| **Workflow State** | ECS Task mem√≥ria | Nincs (request scope) | Nem |
| **Cache** | ECS Task mem√≥ria | Nincs (restart = elv√©sz) | Nem (task-onk√©nt k√ºl√∂n) |
| **Cost Tracker** | ECS Task mem√≥ria | Nincs | Nem |
| **Logs** | CloudWatch Logs | Igen (7 nap retention) | Igen |
| **Metrics** | Prometheus ‚Üí CloudWatch | Igen | Igen |

**Konfigur√°ci√≥s p√©lda - `terraform/ecs.tf`:**

```terraform
resource "aws_ecs_task_definition" "app" {
  family                   = "${var.project_name}-task"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "512"     # 0.5 vCPU
  memory                   = "1024"    # 1 GB RAM
  
  # ‚Üê √ÅLLAPOT T√ÅROL√ÅS ITT (container mem√≥ria)
  
  container_definitions = jsonencode([
    {
      name  = "app"
      image = "${aws_ecr_repository.app.repository_url}:latest"
      
      # Mem√≥ria limit - ha t√∫ll√©pi, OOMKilled
      memory = 512  # MB
      
      environment = [
        {
          name  = "CACHE_TTL_SECONDS"
          value = "3600"  # Cache TTL
        },
        {
          name  = "CACHE_MAX_SIZE"
          value = "1000"  # Max cache entries
        }
      ]
      
      # Logs CloudWatch-ba (perzisztens)
      logConfiguration = {
        logDriver = "awslogs"
        options = {
          "awslogs-group"         = "/ecs/ai-agent/app"
          "awslogs-region"        = "us-east-1"
          "awslogs-stream-prefix" = "app"
        }
      }
    }
  ])
}

resource "aws_ecs_service" "main" {
  name            = "${var.project_name}-service"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.app.arn
  desired_count   = 2  # ‚Üê 2 TASK = 2 k√ºl√∂n mem√≥ria space!
  launch_type     = "FARGATE"
  
  # Load balancer distribution
  load_balancer {
    target_group_arn = aws_lb_target_group.app.arn
    container_name   = "app"
    container_port   = 8000
  }
}
```

**K√∂vetkezm√©nyek:**

1. **Nincs shared cache**: Task 1 √©s Task 2 k√ºl√∂n cache-t haszn√°l
   - Ugyanaz a k√©rd√©s Task 1-en: cache HIT
   - Ugyanaz a k√©rd√©s Task 2-en: cache MISS (√∫jra LLM h√≠v√°s)

2. **Restart = data loss**: Task restart/redeploy t√∂rli a cache-t

3. **Horizontal scaling probl√©m√°k**: T√∂bb task = t√∂bbsz√∂r√∂s cache redundancia

### 5.2 Production-ready Alternat√≠va: Redis Cache

**Architekt√∫ra Redis-szel:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Application Load Balancer             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ECS Service     ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ             ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
  ‚îÇTask 1‚îÇ      ‚îÇTask 2‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò
      ‚îÇ             ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚îÇ TCP 6379
             ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ElastiCache Redis   ‚îÇ  ‚Üê SHARED CACHE
   ‚îÇ   (Managed Service)   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Persistence
         ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  RDB/AOF  ‚îÇ  ‚Üê Redis snapshots
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Terraform konfigur√°ci√≥ (p√©lda):**

```terraform
# terraform/elasticache.tf

resource "aws_elasticache_cluster" "redis" {
  cluster_id           = "${var.project_name}-redis"
  engine               = "redis"
  engine_version       = "7.0"
  node_type            = "cache.t3.micro"  # 0.5 GB RAM
  num_cache_nodes      = 1
  parameter_group_name = "default.redis7"
  port                 = 6379
  
  subnet_group_name  = aws_elasticache_subnet_group.redis.name
  security_group_ids = [aws_security_group.redis.id]
  
  # Snapshot configuration
  snapshot_retention_limit = 5
  snapshot_window         = "03:00-05:00"
  
  tags = {
    Name = "${var.project_name}-shared-cache"
  }
}

# Security Group - csak ECS tasks f√©rhetnek hozz√°
resource "aws_security_group" "redis" {
  name   = "${var.project_name}-redis-sg"
  vpc_id = aws_vpc.main.id
  
  ingress {
    description     = "Redis from ECS"
    from_port       = 6379
    to_port         = 6379
    protocol        = "tcp"
    security_groups = [aws_security_group.ecs_tasks.id]
  }
  
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Environment variable injection
resource "aws_ecs_task_definition" "app" {
  # ...
  
  container_definitions = jsonencode([
    {
      name  = "app"
      # ...
      
      environment = [
        {
          name  = "REDIS_URL"
          value = "redis://${aws_elasticache_cluster.redis.cache_nodes[0].address}:6379"
        },
        {
          name  = "CACHE_BACKEND"
          value = "redis"  # "memory" vagy "redis"
        }
      ]
    }
  ])
}
```

**App konfigur√°ci√≥ update - `app/config.py`:**

```python
class Settings(BaseSettings):
    # ... megl√©v≈ë be√°ll√≠t√°sok ...
    
    # Cache backend v√°laszt√°s
    cache_backend: str = "memory"  # "memory" vagy "redis"
    redis_url: Optional[str] = None
    
    # Cache konfigur√°ci√≥
    cache_ttl_seconds: int = 3600
    cache_max_size: int = 1000
```

**Cache factory - `app/cache/factory.py`:**

```python
"""
Cache factory for creating appropriate cache backend.
"""
from app.cache.interfaces import Cache
from app.cache.memory_cache import MemoryCache
from app.cache.redis_cache import RedisCache
from app.config import settings


def create_cache() -> Cache:
    """
    Create cache instance based on configuration.
    
    Returns:
        Cache implementation (Memory or Redis)
    """
    if settings.cache_backend == "redis":
        if not settings.redis_url:
            raise ValueError("REDIS_URL required for redis backend")
        
        return RedisCache(
            redis_url=settings.redis_url,
            default_ttl_seconds=settings.cache_ttl_seconds
        )
    else:
        # Default: memory cache
        return MemoryCache(
            default_ttl_seconds=settings.cache_ttl_seconds,
            max_size=settings.cache_max_size
        )
```

### 5.3 DynamoDB State Storage (Conversation Persistence)

**Mikor haszn√°ljuk:** Multi-turn conversations, session history

**Schema design:**

```terraform
# terraform/dynamodb.tf - NEM IMPLEMENT√ÅLT (p√©lda)

resource "aws_dynamodb_table" "conversation_sessions" {
  name           = "${var.project_name}-sessions"
  billing_mode   = "PAY_PER_REQUEST"  # On-demand pricing
  hash_key       = "thread_id"
  
  attribute {
    name = "thread_id"
    type = "S"  # String
  }
  
  attribute {
    name = "user_id"
    type = "S"
  }
  
  # GSI for user queries
  global_secondary_index {
    name            = "UserIdIndex"
    hash_key        = "user_id"
    projection_type = "ALL"
  }
  
  ttl {
    attribute_name = "expires_at"
    enabled        = true
  }
  
  tags = {
    Name = "${var.project_name}-conversation-sessions"
  }
}

resource "aws_dynamodb_table" "agent_checkpoints" {
  name         = "${var.project_name}-checkpoints"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "thread_id"
  range_key    = "checkpoint_id"
  
  attribute {
    name = "thread_id"
    type = "S"
  }
  
  attribute {
    name = "checkpoint_id"
    type = "S"
  }
  
  ttl {
    attribute_name = "expires_at"
    enabled        = true
  }
}
```

**LangGraph DynamoDB Checkpointer (elm√©leti):**

```python
# NEM IMPLEMENT√ÅLT - csak p√©lda

from langgraph.checkpoint.dynamodb import DynamoDBSaver

# DynamoDB checkpointer l√©trehoz√°sa
checkpointer = DynamoDBSaver(
    table_name="ai-agent-checkpoints",
    region_name="us-east-1"
)

# Graph compile checkpointer-rel
app = workflow.compile(checkpointer=checkpointer)

# Thread-based conversation
config = {"configurable": {"thread_id": "user_123"}}

# Els≈ë √ºzenet
result1 = await app.ainvoke(
    {"user_input": "What is AI?"},
    config=config
)
# State automatikusan DynamoDB-be mentve!

# M√°sodik √ºzenet - el≈ëz≈ë context bet√∂ltve
result2 = await app.ainvoke(
    {"user_input": "Explain more about neural networks"},
    config=config  # Ugyanaz a thread_id
)
# Az agent "eml√©kszik" az el≈ëz≈ë besz√©lget√©sre!
```

---

## 6. K√≥d P√©ld√°k √©s Konfigur√°ci√≥

### 6.1 Teljes State Lifecycle

**API Request ‚Üí State Creation ‚Üí Node Execution ‚Üí Response**

```python
# app/main.py - teljes flow

@app.post("/run", response_model=RunResponse)
async def run_agent(request: RunRequest):
    """
    Run agent workflow.
    
    √ÅLLAPOT LIFECYCLE:
    1. Initial state creation (√ºres mez≈ëkkel)
    2. Graph execution (node-ok friss√≠tik)
    3. Final state visszaad√°sa
    4. State megsemmis√ºl (GC)
    """
    
    # === 1. INITIAL STATE CREATION ===
    initial_state: AgentState = {
        # Input
        "user_input": request.user_input,
        "scenario": request.scenario,
        
        # Empty outputs (None)
        "classification": None,
        "retrieved_docs": [],
        "retrieval_context": None,
        "reasoning_output": None,
        "final_answer": None,
        
        # Empty metadata
        "nodes_executed": [],
        "models_used": [],
        "timings": {},
        "cache_hits": {}
    }
    
    # === 2. GRAPH EXECUTION ===
    # √öj cost tracker minden k√©r√©shez (stateless!)
    cost_tracker = CostTracker(model_selector)
    
    # Graph l√©trehoz√°s injected dependencies-szel
    graph = create_agent_graph(
        llm_client=llm_client,
        model_selector=model_selector,
        cost_tracker=cost_tracker,
        node_cache=node_cache,        # ‚Üê Shared cache (app-wide)
        embedding_cache=embedding_cache
    )
    
    # Graph futtat√°s (state node-ok k√∂z√∂tt halad)
    start_time = time.time()
    final_state = await graph.ainvoke(initial_state)
    execution_time = time.time() - start_time
    
    # === 3. COST REPORT ===
    cost_report = cost_tracker.get_report()
    
    # === 4. RESPONSE BUILDING ===
    response = RunResponse(
        answer=final_state.get("final_answer", "No answer"),
        debug={
            "nodes_executed": final_state.get("nodes_executed", []),
            "models_used": final_state.get("models_used", []),
            "timings": final_state.get("timings", {}),
            "cache_hits": final_state.get("cache_hits", {}),
            "cost_report": {
                "total_cost_usd": cost_report.total_cost_usd,
                "total_input_tokens": cost_report.total_input_tokens,
                "total_output_tokens": cost_report.total_output_tokens,
                "by_node": {
                    name: {
                        "cost_usd": node.cost_usd,
                        "tokens": node.input_tokens + node.output_tokens
                    }
                    for name, node in cost_report.by_node.items()
                }
            }
        }
    )
    
    # === 5. STATE CLEANUP ===
    # final_state GC √°ltal megsemmis√≠tve (Python)
    # cost_tracker GC √°ltal megsemmis√≠tve
    
    return response
```

### 6.2 Node State Management P√©lda

```python
# app/nodes/reasoning_node.py - r√©szlet

class ReasoningNode:
    """Reasoning node with state management."""
    
    async def execute(self, state: AgentState) -> Dict:
        """
        Execute reasoning node.
        
        STATE OLVAS√ÅS:
        - user_input (triage-t≈ël)
        - classification (triage-t≈ël)
        - retrieval_context (retrieval-t≈ël)
        
        STATE √çR√ÅS:
        - reasoning_output
        - nodes_executed friss√≠t√©s
        - models_used friss√≠t√©s
        - timings friss√≠t√©s
        """
        start_time = time.time()
        
        # ===== STATE OLVAS√ÅS =====
        user_input = state["user_input"]
        classification = state.get("classification", "unknown")
        retrieval_context = state.get("retrieval_context", "")
        
        # ===== LLM H√çV√ÅS =====
        prompt = f"""
        User Question: {user_input}
        Query Type: {classification}
        Context: {retrieval_context}
        
        Provide a detailed answer.
        """
        
        model = self.model_selector.get_model_name(ModelTier.EXPENSIVE)
        response = await self.llm_client.complete(prompt, model)
        
        # ===== COST TRACKING =====
        self.cost_tracker.track_usage(
            node_name="reasoning",
            model=model,
            input_tokens=response.input_tokens,
            output_tokens=response.output_tokens
        )
        
        execution_time = time.time() - start_time
        
        # ===== STATE FRISS√çT√âS (MERGE) =====
        return {
            # √öj output
            "reasoning_output": response.content,
            
            # Metadata friss√≠t√©s (append)
            "nodes_executed": state.get("nodes_executed", []) + ["reasoning"],
            "models_used": state.get("models_used", []) + [model],
            
            # Timing update (merge)
            "timings": {
                **state.get("timings", {}),
                "reasoning": execution_time
            },
            
            # Cache info (no cache for reasoning)
            "cache_hits": {
                **state.get("cache_hits", {}),
                "reasoning": False
            }
        }
```

---

## 7. Best Practices

### 7.1 Stateless Design

‚úÖ **DO:** Minden k√©r√©s f√ºggetlen
```python
# Minden k√©r√©shez √∫j cost tracker
cost_tracker = CostTracker(model_selector)
```

‚ùå **DON'T:** Global state megoszt√°sa k√©r√©sek k√∂z√∂tt
```python
# ROSSZ - race condition!
global_cost_tracker = CostTracker(...)  # NE!
```

### 7.2 Cache Strat√©gia

‚úÖ **DO:** Determinisztikus cache kulcsok
```python
cache_key = generate_cache_key("triage", user_input)
# Ugyanaz az input -> ugyanaz a kulcs
```

‚úÖ **DO:** Megfelel≈ë TTL be√°ll√≠t√°s
```python
# Gyors v√°ltoz√≥ adatok: r√∂vid TTL
cache.set(key, value, ttl_seconds=300)  # 5 perc

# Stabil adatok: hossz√∫ TTL
cache.set(key, value, ttl_seconds=3600)  # 1 √≥ra
```

‚ùå **DON'T:** T√∫l nagy cache entries
```python
# ROSSZ - mem√≥ria probl√©ma!
await cache.set(key, huge_object)  # MB-os objektum
```

### 7.3 Error Handling

‚úÖ **DO:** Graceful degradation cache failure eset√©n
```python
try:
    cached = await cache.get(key)
except Exception as e:
    logger.warning(f"Cache error: {e}")
    cached = None  # Continue without cache
```

### 7.4 Monitoring

‚úÖ **DO:** State metadata tracking
```python
return {
    "nodes_executed": [...],
    "timings": {...},
    "cache_hits": {...}
}
```

‚úÖ **DO:** Prometheus metrics
```python
metrics.cache_hit_total.labels(cache="node", node="triage").inc()
```

---

## √ñsszefoglal√°s

| K√©rd√©s | V√°lasz |
|--------|--------|
| **Hol van az √°llapot t√°rolva lok√°lisan?** | Python mem√≥ria (Dict objektumok) |
| **Hol van az √°llapot t√°rolva AWS-ben?** | ECS Task mem√≥ria (nem perzisztens) |
| **Milyen √°llapot t√≠pusok vannak?** | 1) Workflow state (request scope)<br>2) Cache state (TTL-based)<br>3) Persistent state (NEM haszn√°lt) |
| **Hogyan osztott az √°llapot?** | Cache: app-wide (de task-onk√©nt k√ºl√∂n)<br>Workflow: request-specific |
| **Mi t√∂rt√©nik restart eset√©n?** | Workflow state: elv√©sz (OK, stateless)<br>Cache: elv√©sz (√∫jra kell √©p√≠teni) |
| **Shared cache production-ben?** | Redis/ElastiCache sz√ºks√©ges |
| **Conversation history t√°mogat√°s?** | NEM - minden k√©r√©s f√ºggetlen |

**Kulcs Tanuls√°g:**
- Jelenlegi rendszer: **Stateless, memory-based, request-scoped**
- Production upgrade: **Redis cache megoszt√°shoz**
- Multi-turn: **DynamoDB + LangGraph checkpointer**

---

## 8. AI Agent Deploy Checklist - Implement√°ci√≥ Ellen≈ërz√©s

### Mi k√ºl√∂nb√∂zteti meg az AI Agent Deploy-t egy hagyom√°nyos backendt≈ël?

**AI Agent = Szoftver + D√∂nt√©si Logika + K√∂lts√©g**

Az al√°bbi checklist minden speci√°lis kih√≠v√°st tartalmaz, √©s pontosan le√≠rja, hogy **mi biztos√≠tja** a megold√°st ebben az alkalmaz√°sban.

---

### ‚úÖ 1. LLM API Kulcsok Kezel√©se

**K√∂vetelm√©ny:** Biztons√°gos API kulcs t√°rol√°s k√ºl√∂nb√∂z≈ë k√∂rnyezetekben (dev, staging, prod).

#### üîß Implement√°ci√≥ ebben az app-ban:

| K√∂rnyezet | Megold√°s | F√°jl/Konfigur√°ci√≥ |
|-----------|----------|-------------------|
| **Development** | `.env` f√°jl (gitignore-olva) | `.env` |
| **Konfigur√°ci√≥ bet√∂lt√©s** | Pydantic Settings | `app/config.py` |
| **Docker** | `env_file` docker-compose-ban | `docker-compose.yml` |
| **Production (AWS)** | Environment variables ECS Task Definition-ben | `terraform/ecs.tf` |
| **Alternat√≠va (Best Practice)** | AWS Secrets Manager | `terraform/ecs.tf` (kommentben p√©lda) |

#### üìù Konkr√©t K√≥d:

**`app/config.py` - Pydantic Settings:**
```python
class Settings(BaseSettings):
    openai_api_key: Optional[str] = None
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```

**`docker-compose.yml` - Docker k√∂rnyezet:**
```yaml
services:
  agent-demo:
    env_file:
      - .env  # API kulcsok itt
```

**`terraform/ecs.tf` - Production (AWS ECS):**
```terraform
container_definitions = jsonencode([
  {
    environment = [
      {
        name  = "OPENAI_API_KEY"
        value = var.openai_api_key  # GitHub Secrets ‚Üí Terraform
      }
    ]
  }
])
```

**Alternat√≠va - AWS Secrets Manager:**
```terraform
secrets = [
  {
    name      = "OPENAI_API_KEY"
    valueFrom = aws_secretsmanager_secret.openai_key.arn
  }
]
```

**‚úÖ Ellen≈ërz√©s:** 
- [ ] `.env` f√°jl l√©tezik √©s `.gitignore`-ban van
- [ ] `app/config.py` tartalmazza az `openai_api_key` mez≈ët
- [ ] Docker Compose haszn√°lja `env_file`-t
- [ ] Terraform ECS task definition tartalmazza az environment v√°ltoz√≥t

---

### ‚úÖ 2. Nem Determinisztikus Fut√°s

**K√∂vetelm√©ny:** LLM nem determinisztikus ‚Üí ugyanaz a prompt k√ºl√∂nb√∂z≈ë v√°laszokat adhat. Sz√ºks√©ges: mock client tesztel√©shez, cache konzisztenci√°hoz, r√©szletes logging.

#### üîß Implement√°ci√≥ ebben az app-ban:

| Megold√°s | C√©lja | F√°jl |
|----------|-------|------|
| **Mock LLM Client** | Determinisztikus v√°laszok tesztel√©shez | `app/llm/mock_client.py` |
| **MemoryCache** | Ugyanaz az input ‚Üí ugyanaz a v√°lasz (TTL-en bel√ºl) | `app/cache/memory_cache.py` |
| **Cache kulcs gener√°l√°s** | Hash-alap√∫ kulcsok | `app/cache/keys.py` |
| **Structured logging** | Minden LLM h√≠v√°s napl√≥zva | `app/logging_conf.py` |
| **Metrics** | Prometheus observability | `app/observability/metrics.py` |

#### üìù Konkr√©t K√≥d:

**`app/llm/mock_client.py` - Determinisztikus mock:**
```python
class MockLLMClient(LLMClient):
    async def complete(self, prompt: str, model: str) -> LLMResponse:
        # Determinisztikus v√°laszok keyword alapj√°n
        if "classify" in prompt.lower():
            content = "simple"
        elif "retrieve" in prompt.lower():
            content = "Retrieved documents: [Mock Doc 1, Mock Doc 2]"
        # ...
        return LLMResponse(content=content, ...)
```

**`app/cache/memory_cache.py` - Cache konzisztencia:**
```python
class MemoryCache:
    async def get(self, key: str) -> Optional[Any]:
        # Ugyanaz a kulcs ‚Üí ugyanaz az √©rt√©k (TTL-en bel√ºl)
        entry = self._store.get(key)
        if entry and time.time() <= entry.expires_at:
            return entry.value
        return None
```

**`app/cache/keys.py` - Determinisztikus cache kulcs:**
```python
def generate_cache_key(node_name: str, *args: Any) -> str:
    serialized = json.dumps(args, sort_keys=True)
    hash_digest = hashlib.sha256(serialized.encode()).hexdigest()
    return f"{node_name}:{hash_digest[:16]}"
```

**`app/main.py` - Mock vs OpenAI v√°laszt√°s:**
```python
if settings.openai_api_key:
    llm_client = OpenAIClient(api_key=settings.openai_api_key)
else:
    llm_client = MockLLMClient(latency_ms=100)
```

**‚úÖ Ellen≈ërz√©s:**
- [ ] `MockLLMClient` l√©tezik √©s determinisztikus v√°laszokat ad
- [ ] `MemoryCache` implement√°lja a `Cache` interface-t
- [ ] `generate_cache_key` hash-alap√∫
- [ ] Node-ok haszn√°lj√°k a cache-t (pl. `triage_node.py`)
- [ ] Logging be√°ll√≠tva (`app/logging_conf.py`)

---

### ‚úÖ 3. K√ºls≈ë Toolok √©s API-k

**K√∂vetelm√©ny:** LLM client, cache, embedding service dependency-k kezel√©se. Interface-alap√∫ architekt√∫ra a cser√©lhet≈ës√©ghez.

#### üîß Implement√°ci√≥ ebben az app-ban:

| Pattern | C√©lja | F√°jl |
|---------|-------|------|
| **Interface Pattern (ABC)** | F√ºgg≈ës√©gek absztrakci√≥ja | `app/llm/interfaces.py`, `app/cache/interfaces.py` |
| **Dependency Injection** | Node-ok kapj√°k a dependency-ket | `app/graph/agent_graph.py` |
| **Factory Pattern** | Graph l√©trehoz√°s DI-vel | `app/graph/agent_graph.py` (AgentGraphFactory) |

#### üìù Konkr√©t K√≥d:

**`app/llm/interfaces.py` - LLM Interface:**
```python
from abc import ABC, abstractmethod

class LLMClient(ABC):
    @abstractmethod
    async def complete(self, prompt: str, model: str) -> LLMResponse:
        pass
```

**`app/cache/interfaces.py` - Cache Interface:**
```python
class Cache(ABC):
    @abstractmethod
    async def get(self, key: str) -> Optional[Any]:
        pass
    
    @abstractmethod
    async def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None):
        pass
```

**`app/graph/agent_graph.py` - Dependency Injection:**
```python
class AgentGraphFactory:
    def __init__(
        self,
        llm_client: LLMClient,      # Interface
        model_selector: ModelSelector,
        cost_tracker: CostTracker,
        node_cache: Cache,          # Interface
        embedding_cache: Cache
    ):
        # Dependency-k t√°rol√°sa
        
    def create_graph(self):
        # Node-ok l√©trehoz√°sa injected dependency-kkel
        triage_node = TriageNode(
            llm_client=self.llm_client,
            cache=self.node_cache,
            # ...
        )
```

**`app/main.py` - Composition Root:**
```python
# Dependency-k l√©trehoz√°sa
llm_client = OpenAIClient(...) or MockLLMClient(...)
node_cache = MemoryCache(...)
model_selector = ModelSelector()
cost_tracker = CostTracker(model_selector)

# Graph factory
graph = create_agent_graph(
    llm_client=llm_client,
    node_cache=node_cache,
    # ...
)
```

**‚úÖ Ellen≈ërz√©s:**
- [ ] `LLMClient` ABC l√©tezik interface-k√©nt
- [ ] `Cache` ABC l√©tezik interface-k√©nt
- [ ] `AgentGraphFactory` haszn√°l dependency injection-t
- [ ] Node-ok constructor-ban kapj√°k a dependency-ket
- [ ] `main.py` a "composition root" (√∂sszek√∂ti a dependency-ket)

---

### ‚úÖ 4. √Ållapot (State, Memory)

**K√∂vetelm√©ny:** T√∂bbl√©pcs≈ës workflow √°llapotkezel√©se, node-ok k√∂z√∂tti adat√°tad√°s, metadata tracking.

#### üîß Implement√°ci√≥ ebben az app-ban:

| Komponens | C√©lja | F√°jl |
|-----------|-------|------|
| **TypedDict State** | T√≠pusbiztos √°llapot defin√≠ci√≥ | `app/graph/state.py` |
| **LangGraph StateGraph** | State management workflow-ban | `app/graph/agent_graph.py` |
| **State merge** | Node-ok friss√≠tik a state-t | Minden node (`app/nodes/*.py`) |
| **Metadata tracking** | Fut√°si adatok (timing, cost, cache) | `app/graph/state.py` (AgentState) |
| **MemoryCache** | Node-level caching | `app/cache/memory_cache.py` |

#### üìù Konkr√©t K√≥d:

**`app/graph/state.py` - State defin√≠ci√≥:**
```python
class AgentState(TypedDict, total=False):
    # Input
    user_input: str
    scenario: Optional[str]
    
    # Node outputs
    classification: Optional[str]
    retrieved_docs: List[str]
    retrieval_context: Optional[str]
    reasoning_output: Optional[str]
    final_answer: Optional[str]
    
    # Metadata
    nodes_executed: List[str]
    models_used: List[str]
    timings: Dict[str, float]
    cache_hits: Dict[str, bool]
```

**`app/graph/agent_graph.py` - StateGraph haszn√°lat:**
```python
workflow = StateGraph(AgentState)
workflow.add_node("triage", triage_node.execute)
workflow.add_node("retrieval", retrieval_node.execute)
# ...
app = workflow.compile()
```

**`app/nodes/triage_node.py` - State friss√≠t√©s:**
```python
async def execute(self, state: AgentState) -> Dict:
    user_input = state["user_input"]  # Olvas√°s
    # ... LLM h√≠v√°s ...
    
    return {
        "classification": classification,  # √öj mez≈ë
        "nodes_executed": state.get("nodes_executed", []) + ["triage"],
        "timings": {**state.get("timings", {}), "triage": exec_time}
    }
```

**`app/main.py` - Initial state creation:**
```python
initial_state: AgentState = {
    "user_input": request.user_input,
    "scenario": request.scenario,
    "classification": None,
    "nodes_executed": [],
    "timings": {},
    # ...
}

final_state = await graph.ainvoke(initial_state)
```

**T√°rol√°s t√≠pusok:**

| √Ållapot T√≠pus | T√°rol√°s Helye | Persistence | Scope |
|---------------|---------------|-------------|-------|
| **Workflow State** | Python mem√≥ria | Request scope | Egy k√©r√©s |
| **Cache State** | `MemoryCache._store` Dict | TTL (1h) | App-wide |
| **Logs** | CloudWatch Logs (AWS) | 7 nap | Global |
| **Metrics** | Prometheus ‚Üí Grafana | Id≈ëb√©lyeg szerint | Global |

**AWS Production - `terraform/ecs.tf`:**
```terraform
resource "aws_ecs_task_definition" "app" {
  cpu    = "512"   # 0.5 vCPU
  memory = "1024"  # 1 GB RAM ‚Üê √ÅLLAPOT ITT (container mem√≥ria)
}
```

**‚úÖ Ellen≈ërz√©s:**
- [ ] `AgentState` TypedDict l√©tezik
- [ ] `StateGraph` haszn√°lja az `AgentState`-t
- [ ] Node-ok return Dict-tel friss√≠tik a state-t
- [ ] Initial state l√©trehoz√°s `main.py`-ban
- [ ] Metadata tracking (nodes_executed, timings, cache_hits)
- [ ] ECS task defin√≠ci√≥ tartalmaz mem√≥ria limitet

---

### ‚úÖ 5. K√∂lts√©g / Token Usage

**K√∂vetelm√©ny:** LLM h√≠v√°sok token-alap√∫ k√∂lts√©geinek nyomon k√∂vet√©se, node-onk√©nti √©s model-enk√©nti breakdown, Prometheus metrics.

#### üîß Implement√°ci√≥ ebben az app-ban:

| Komponens | C√©lja | F√°jl |
|-----------|-------|------|
| **CostTracker** | Token √©s USD tracking | `app/llm/cost_tracker.py` |
| **ModelSelector** | Model pricing lookup | `app/llm/models.py` |
| **Prometheus Metrics** | Token √©s cost counter-ek | `app/observability/metrics.py` |
| **Grafana Dashboard** | Cost vizualiz√°ci√≥ | `grafana/dashboards/agent-dashboard.json` |
| **API Response** | Cost report minden k√©r√©sben | `app/main.py` (RunResponse) |

#### üìù Konkr√©t K√≥d:

**`app/llm/cost_tracker.py` - Cost tracking:**
```python
class CostTracker:
    def track_usage(
        self,
        node_name: str,
        model: str,
        input_tokens: int,
        output_tokens: int
    ):
        # Pricing lek√©r√©se
        input_price, output_price = self.model_selector.get_pricing(model)
        
        # K√∂lts√©g sz√°m√≠t√°s (USD / 1K token)
        cost = (
            (input_tokens / 1000.0) * input_price +
            (output_tokens / 1000.0) * output_price
        )
        
        # Track by node √©s by model
        self._node_costs[node_name].cost_usd += cost
        self._model_costs[model].cost_usd += cost
```

**`app/llm/models.py` - Pricing konfigur√°ci√≥:**
```python
class ModelSelector:
    def get_pricing(self, model: str) -> Tuple[float, float]:
        # Input √©s output pricing (USD / 1K token)
        pricing_map = {
            "gpt-3.5-turbo": (0.0001, 0.0002),
            "gpt-4-turbo-preview": (0.001, 0.002),
            "gpt-4": (0.01, 0.03)
        }
        return pricing_map.get(model, (0.0, 0.0))
```

**`app/config.py` - Pricing konfigur√°ci√≥:**
```python
class Settings(BaseSettings):
    # Pricing (USD per 1K tokens)
    price_cheap_input: float = 0.0001
    price_cheap_output: float = 0.0002
    price_medium_input: float = 0.001
    price_medium_output: float = 0.002
    price_expensive_input: float = 0.01
    price_expensive_output: float = 0.03
```

**`app/observability/metrics.py` - Prometheus metrics:**
```python
llm_inference_token_input_total = Counter(
    'llm_inference_token_input_total',
    'Total input tokens consumed',
    ['model', 'node']
)

llm_inference_token_output_total = Counter(
    'llm_inference_token_output_total',
    'Total output tokens generated',
    ['model', 'node']
)

llm_cost_total_usd = Counter(
    'llm_cost_total_usd',
    'Total LLM cost in USD',
    ['model', 'node']
)
```

**Node-okban haszn√°lat - pl. `app/nodes/reasoning_node.py`:**
```python
# LLM h√≠v√°s ut√°n
self.cost_tracker.track_usage(
    node_name="reasoning",
    model=model,
    input_tokens=response.input_tokens,
    output_tokens=response.output_tokens
)

# Prometheus metrics
metrics.llm_cost_total_usd.labels(
    model=model,
    node="reasoning"
).inc(cost_usd)
```

**`app/main.py` - Cost report API v√°laszban:**
```python
cost_report = cost_tracker.get_report()

response = RunResponse(
    answer=final_state.get("final_answer"),
    debug={
        "cost_report": {
            "total_cost_usd": cost_report.total_cost_usd,
            "total_input_tokens": cost_report.total_input_tokens,
            "total_output_tokens": cost_report.total_output_tokens,
            "by_node": {...},
            "by_model": {...}
        }
    }
)
```

**‚úÖ Ellen≈ërz√©s:**
- [ ] `CostTracker` implement√°lva
- [ ] `ModelSelector` tartalmazza a pricing map-et
- [ ] `app/config.py` tartalmazza a pricing konstansokat
- [ ] Prometheus metrics defini√°lva (`llm_cost_total_usd` stb.)
- [ ] Node-ok h√≠vj√°k a `cost_tracker.track_usage()`-t
- [ ] API v√°lasz tartalmaz cost report-ot
- [ ] Grafana dashboard l√©tezik (`grafana/dashboards/`)

---

### ‚úÖ 6. Verzi√≥z√°s (Prompt ‚â† K√≥d)

**K√∂vetelm√©ny:** Prompt-ok verzi√≥k√∂vet√©se, mivel az AI viselked√©se nem csak a k√≥dt√≥l, hanem a prompt-okt√≥l is f√ºgg.

#### üîß Implement√°ci√≥ ebben az app-ban:

| Megold√°s | C√©lja | F√°jl/Mappa |
|----------|-------|------------|
| **Prompt f√°jlok** | K√ºl√∂n f√°jlokban t√°rolva | `prompts/*.txt` |
| **Git tracking** | Prompt v√°ltoz√°sok k√∂vethet≈ëk | `.git/` (commit history) |
| **Template loading** | Runtime bet√∂lt√©s | Node-ok (pl. `app/nodes/triage_node.py`) |
| **Prompt hash tracking** | Verzi√≥ metadata | `app/graph/state.py` (prompt_versions) |

#### üìù Konkr√©t K√≥d:

**Prompt f√°jlok strukt√∫ra:**
```
prompts/
‚îú‚îÄ‚îÄ triage_prompt.txt       ‚Üê Klasszifik√°ci√≥s prompt
‚îú‚îÄ‚îÄ retrieval_prompt.txt    ‚Üê Retrieval prompt
‚îú‚îÄ‚îÄ reasoning_prompt.txt    ‚Üê Reasoning prompt
‚îî‚îÄ‚îÄ summary_prompt.txt      ‚Üê Summary prompt
```

**`prompts/triage_prompt.txt` - p√©lda:**
```text
Classify the query type. Output ONE word only.

Types:
- simple: factual, direct answer
- retrieval: requires looking up information
- complex: needs reasoning or analysis

Query: {user_input}

Classification:
```

**Node-okban prompt bet√∂lt√©s - pl. `app/nodes/triage_node.py`:**
```python
from pathlib import Path

PROMPT_DIR = Path(__file__).parent.parent.parent / "prompts"
TRIAGE_PROMPT_PATH = PROMPT_DIR / "triage_prompt.txt"

def _load_prompt(self) -> str:
    """Load prompt template from file."""
    with open(TRIAGE_PROMPT_PATH, 'r') as f:
        return f.read()

async def execute(self, state: AgentState) -> Dict:
    # Prompt bet√∂lt√©s
    prompt_template = self._load_prompt()
    
    # V√°ltoz√≥k behelyettes√≠t√©se
    prompt = prompt_template.format(user_input=state["user_input"])
    
    # LLM h√≠v√°s
    response = await self.llm_client.complete(prompt, model)
```

**Git commit history:**
```bash
git log --oneline prompts/reasoning_prompt.txt

# Output:
# a3f2e1d Update reasoning prompt to be more specific
# 7b8c9d2 Add context usage instruction
# 1e5f6a3 Initial reasoning prompt
```

**Prompt verzi√≥ tracking (opcion√°lis - nem implement√°lt, de p√©lda):**
```python
import hashlib

def get_prompt_hash(prompt: str) -> str:
    """SHA256 hash for versioning."""
    return hashlib.sha256(prompt.encode()).hexdigest()[:8]

# Haszn√°lat node-ban
prompt_hash = get_prompt_hash(prompt_template)

return {
    "reasoning_output": response.content,
    "prompt_versions": {
        **state.get("prompt_versions", {}),
        "reasoning": prompt_hash
    }
}
```

**‚úÖ Ellen≈ërz√©s:**
- [ ] `prompts/` mappa l√©tezik
- [ ] Minden node-hoz tartozik `.txt` prompt f√°jl
- [ ] Prompt-ok Git-ben k√∂vetve vannak
- [ ] Node-ok bet√∂ltik a prompt-okat f√°jlb√≥l
- [ ] Prompt v√°ltoz√°sok k√ºl√∂n commit-ban vannak
- [ ] (Opcion√°lis) Prompt hash tracking implement√°lva

---

### ‚úÖ 7. Monitoring √©s Observability

**K√∂vetelm√©ny:** Val√≥s idej≈± monitoring, metrics, dashboardok az LLM k√∂lts√©gek, latency, cache hat√©konys√°g k√∂vet√©s√©re.

#### üîß Implement√°ci√≥ ebben az app-ban:

| Komponens | C√©lja | F√°jl/Service |
|-----------|-------|--------------|
| **Prometheus** | Metrics gy≈±jt√©s | `prometheus/prometheus.yml`, Docker service |
| **Grafana** | Dashboardok | `grafana/`, Docker service |
| **Metrics endpoint** | `/metrics` API | `app/main.py` |
| **Custom metrics** | LLM, cache, agent metrics | `app/observability/metrics.py` |
| **Middleware** | Request tracking | `app/observability/middleware.py` |
| **CloudWatch Logs** | Production logs (AWS) | `terraform/ecs.tf` |

#### üìù Konkr√©t K√≥d:

**`prometheus/prometheus.yml` - Prometheus konfigur√°ci√≥:**
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'agent-demo'
    static_configs:
      - targets: ['agent-demo:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
```

**`docker-compose.yml` - Prometheus √©s Grafana:**
```yaml
services:
  prometheus:
    image: prom/prometheus:v2.48.0
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
  
  grafana:
    image: grafana/grafana:10.2.2
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3000:3000"
```

**`app/observability/metrics.py` - Metrics defin√≠ci√≥k:**
```python
from prometheus_client import Counter, Histogram

# LLM metrics
llm_inference_count_total = Counter(
    'llm_inference_count_total',
    'Total LLM calls',
    ['model', 'node', 'status']
)

llm_cost_total_usd = Counter(
    'llm_cost_total_usd',
    'Total cost in USD',
    ['model', 'node']
)

# Cache metrics
cache_hit_total = Counter(
    'cache_hit_total',
    'Cache hits',
    ['cache', 'node']
)

# Agent metrics
agent_execution_latency_seconds = Histogram(
    'agent_execution_latency_seconds',
    'Agent execution latency',
    ['graph'],
    buckets=[0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]
)
```

**`app/main.py` - Metrics endpoint:**
```python
from prometheus_client import REGISTRY, generate_latest, CONTENT_TYPE_LATEST

@app.get("/metrics")
async def metrics_endpoint():
    """Prometheus metrics endpoint."""
    return Response(
        content=generate_latest(REGISTRY),
        media_type=CONTENT_TYPE_LATEST
    )
```

**`app/observability/middleware.py` - Request tracking:**
```python
class MetricsMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        response = await call_next(request)
        latency = time.time() - start_time
        
        # Track HTTP request metrics
        http_requests_total.labels(
            method=request.method,
            path=request.url.path,
            status=response.status_code
        ).inc()
        
        return response
```

**`app/logging_conf.py` - Structured logging:**
```python
import logging

def setup_logging() -> logging.Logger:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    return logging.getLogger("agent-demo")
```

**AWS CloudWatch - `terraform/ecs.tf`:**
```terraform
resource "aws_cloudwatch_log_group" "app" {
  name              = "/ecs/${var.project_name}/app"
  retention_in_days = 7
}

container_definitions = jsonencode([
  {
    logConfiguration = {
      logDriver = "awslogs"
      options = {
        "awslogs-group"  = aws_cloudwatch_log_group.app.name
        "awslogs-region" = var.aws_region
      }
    }
  }
])
```

**Grafana Dashboard - `grafana/dashboards/agent-dashboard.json`:**
```json
{
  "title": "AI Agent Monitoring",
  "panels": [
    {
      "title": "Total Cost (USD)",
      "targets": [{
        "expr": "sum(llm_cost_total_usd)"
      }]
    },
    {
      "title": "Cache Hit Rate",
      "targets": [{
        "expr": "sum(cache_hit_total) / (sum(cache_hit_total) + sum(cache_miss_total))"
      }]
    }
  ]
}
```

**‚úÖ Ellen≈ërz√©s:**
- [ ] `prometheus/prometheus.yml` l√©tezik √©s konfigur√°lja a scrape-et
- [ ] `docker-compose.yml` tartalmazza Prometheus √©s Grafana service-eket
- [ ] `app/observability/metrics.py` defini√°lja a custom metrics-eket
- [ ] `/metrics` endpoint el√©rhet≈ë (`app/main.py`)
- [ ] Middleware tracking implement√°lva
- [ ] Structured logging be√°ll√≠tva
- [ ] Grafana dashboard JSON l√©tezik
- [ ] (Production) CloudWatch log group l√©trehozva Terraform-mel

---

## 9. Teljes Implement√°ci√≥ Ellen≈ërz≈ë T√°bl√°zat

| # | Kih√≠v√°s | Status | F≈ë Implement√°ci√≥s F√°jlok |
|---|---------|--------|--------------------------|
| 1Ô∏è‚É£ | **LLM API kulcsok** | ‚úÖ | `app/config.py`, `docker-compose.yml`, `terraform/ecs.tf` |
| 2Ô∏è‚É£ | **Nem determinisztikus fut√°s** | ‚úÖ | `app/llm/mock_client.py`, `app/cache/memory_cache.py`, `app/cache/keys.py` |
| 3Ô∏è‚É£ | **K√ºls≈ë toolok √©s API-k** | ‚úÖ | `app/llm/interfaces.py`, `app/cache/interfaces.py`, `app/graph/agent_graph.py` |
| 4Ô∏è‚É£ | **√Ållapot (state, memory)** | ‚úÖ | `app/graph/state.py`, `app/graph/agent_graph.py`, `app/nodes/*.py` |
| 5Ô∏è‚É£ | **K√∂lts√©g / token usage** | ‚úÖ | `app/llm/cost_tracker.py`, `app/observability/metrics.py`, `app/config.py` |
| 6Ô∏è‚É£ | **Verzi√≥z√°s (prompt ‚â† k√≥d)** | ‚úÖ | `prompts/*.txt`, Git commit history |
| 7Ô∏è‚É£ | **Monitoring √©s observability** | ‚úÖ | `prometheus/`, `grafana/`, `app/observability/`, `terraform/ecs.tf` (CloudWatch) |

---

## 10. Production Deployment Checklist (AWS)

### Infrastructure (Terraform)

- [ ] **VPC √©s Networking**: `terraform/vpc.tf`
  - VPC l√©trehozva
  - Public subnet-ek
  - Internet Gateway
  - Route table-k

- [ ] **ECS Fargate**: `terraform/ecs.tf`
  - ECS cluster
  - Task definition (CPU, Memory)
  - ECS service (desired count)
  - Task role √©s execution role

- [ ] **Application Load Balancer**: `terraform/alb.tf`
  - ALB l√©trehozva
  - Target group
  - Listener (HTTP/HTTPS)
  - Health check konfigur√°ci√≥

- [ ] **ECR Repository**: `terraform/ecr.tf`
  - Docker image repository
  - Lifecycle policy

- [ ] **CloudWatch Logs**: `terraform/ecs.tf`
  - Log groups l√©trehozva
  - Retention policy (7 nap)

- [ ] **Environment Variables**: `terraform/ecs.tf`
  - OPENAI_API_KEY injected
  - Cache konfigur√°ci√≥
  - Log level

### Application

- [ ] **Docker Image Build**:
  ```bash
  docker build -t ai-agent-app -f docker/Dockerfile .
  ```

- [ ] **ECR Push**:
  ```bash
  aws ecr get-login-password | docker login --username AWS --password-stdin <ECR_URI>
  docker tag ai-agent-app:latest <ECR_URI>:latest
  docker push <ECR_URI>:latest
  ```

- [ ] **ECS Deployment**:
  ```bash
  terraform apply
  aws ecs update-service --cluster <cluster> --service <service> --force-new-deployment
  ```

### Monitoring

- [ ] **Prometheus**: ECS task-ban fut, scrape-eli az app `/metrics` endpoint-j√°t
- [ ] **Grafana**: ECS task-ban fut, dashboardok provisioned
- [ ] **CloudWatch Logs**: ECS stdout/stderr ‚Üí CloudWatch
- [ ] **Alerts**: (Opcion√°lis) CloudWatch Alarms cost threshold-ra

### Security

- [ ] **API Keys**: Environment variables vagy Secrets Manager
- [ ] **Security Groups**: 
  - ALB: 80/443 nyitva
  - ECS tasks: csak ALB-b≈ël el√©rhet≈ë (8000 port)
- [ ] **IAM Roles**: Least privilege principle

---

## Konkl√∫zi√≥

Ez az alkalmaz√°s **minden speci√°lis AI agent kih√≠v√°st kezel** struktur√°lt, production-ready m√≥don:

‚úÖ **Biztons√°gos API kulcs kezel√©s** - k√∂rnyezet-f√ºgg≈ë konfigur√°ci√≥  
‚úÖ **Determinisztikus tesztel√©s** - mock client + cache  
‚úÖ **Dependency injection** - cser√©lhet≈ë komponensek  
‚úÖ **Stateful workflow** - LangGraph TypedDict state  
‚úÖ **K√∂lts√©g tracking** - real-time token √©s USD monitoring  
‚úÖ **Prompt verzi√≥z√°s** - Git-ben k√∂vetve  
‚úÖ **Production monitoring** - Prometheus + Grafana + CloudWatch  

**Minden f√°jl √©s konfigur√°ci√≥ pontosan dokument√°lva.**

---

**K√©sz√≠tette:** AI Agent Team  
**Verzi√≥:** 1.1 (Checklist hozz√°adva)  
**D√°tum:** 2026-01-22
